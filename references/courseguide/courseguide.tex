\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}

\usepackage[shortlabels, inline]{enumitem}

\title{Courseguide Machine Learning}
\author{Grouls, Raoul \\
    \texttt{raoul.grouls@businessdecision.nl}
    }
\date{\today}

\begin{document}
\maketitle
\section{Common Information}

\begin{table}[h]
    \begin{tabular}{ll}
     Period & Nov 22 - Jan 23 \\
     Subject & Masterclass Machine Learning \\
     Credits & 5 ECTS \\
     Github & \href{https://github.com/raoulg/ML22}{raoulg/ML22}\\
     Language & English and Dutch \\
     Prerequisite & Python programming and visualisation, statistics \\
     Module leader & Raoul Grouls \\
     Phone & 06-26710051 \\
     Email & raoul.grouls@businessdecision.nl
    \end{tabular}
\end{table}

\begin{table}[h]
    \begin{tabular}{ll}
     Lectures (7 times 3 hr)& 21 \\
     Lab (7 times 3 hr)& 21 \\
     Private study (minimum) & 56 \\
     & 
    \end{tabular}
\end{table}

\section{Context}

When you have learned to automate things with python, and you are familiar
with data cleaning (pandas/polars), data visualisation (matplotlib/seaborn)
and you are comfortable with statistics (mean, median, boxplots, histograms, 
probability in general, continuous / discrete distributions, normal distributions,
simpsons paradox) it is quite natural to start creating models.
All these items have been covered in the Data Mining \& Exploration (DME) course.

\par The field of machine learning is really too big to cover everything. 
We have:
\begin{enumerate}[I]
    \item "classic" machine learning from the 80s-00s (linear regression, random forrests, 
    Support Vector Machines) 
    \item probabilistic programming (PyMC, Turing.jl)
    \item deep learning: Neural networks and their variations (CNNs, RNNs, Transformers)
\end{enumerate}

The theory of the classic machine learning algorithms is covered in 'Datascience for Business".
This are the models you can store on a floppy disk, and run on a Commodore 64 (or, if you insist, excel).
Probabilistic Programming is interesting if you need explainable models. It is definitely 
usefull, but this course will focus on deep learning.
We will use the \href{https://pytorch.org/}{Pytorch} framework, which is very usefull for learning 
the concepts and frequently used in a business context.

\section{Environment}
\subsection{Library management}
Reproducing results is really important. Especially on other machines than your own (unless
you are planning to hand over you laptop to every client).
We will be using \href{https://python-poetry.org/}{poetry}, which has been used in DME course.
If you are not familiar, read the documents!
\subsection{Code style guidelines}
We will be following professional \href{https://staticsitecodestyle.z6.web.core.windows.net/}{code style guidelines}. 
A lot will be familiar from DME, and most of the code in this course follows these guidelines, so you 
can get familiar by paying close attention to what is happening.
However, make sure to read through it. If you follow these 
guidelines, everyone that needs to read your code at a later time will thank you, including
yourself 6 months from now...
\subsection{Git}
Because emailing code is not an option (yes I know you \textit{can}, but that doesnt mean it is a good idea).
we will need git, which is industry standard. Again, you have seen this in DME. Please try to practice git,
I know it doesn't has the best user interface, but you will need to get used to it. If you really struggle,
try one of the graphic interfaces or plugins.  
\subsection{Linting}
We will use linters. This is just an additional tool to get clean code. Especially
\href{https://github.com/psf/black}{black}
\subsection{Gin and Ray}
We will need to do a lot of experimentation. It is really to only way to develop an intuition 
about what sane settings are for a model.
\href{https://github.com/google/gin-config}{gin-config} will help us not to loose track of
everything we are doing, by storing configuration in a small textfile. In addition to that,
we will use \href{https://docs.ray.io/en/latest/tune/index.html}{ray tune} to automate searching.
Ray offers algorithms that help us searching through settings-spaces that are too vast to search manually.

\section{Lessons}
We will use the book  `Programming PyTorch for Deep Learning'. 
While it will help us with a lot of lessons, but not all lessons will be covered by the book.

I added a 0-baseline lesson. It has excercises and answers. I created these lessons as
additional stuff during the years, but it is all usefull.

Lessons 1-3 will be covered by the book, lesson 4 will be fully focused on working with ray.

Since the 2017 paper `Attention is all you need', the transformers architecture is the state-of-the-art.
Even though the book doesn't cover it, we will take a full lesson to look at the architecture.

I will also spend some time on unsupervised problems, which is very usefull if you don't have
a labeled dataset available.

And finally, while pytorch is pretty fast, due to it's c++ backend, it can be problematic to use python.
During the DME lessons, we have used polars as a fast alternative for pandas. For the machine
learning course, I explored the \href{https://github.com/google/trax}{trax} library from
google, but they seem to have deprecated the project. That's why I will change that into a lesson to
show you how to build neural networks with julia. Yes, thats another language, but I think
you will find the syntax very readable, and I hope you can appreciate the factor 10-100 speedup compared to python.













\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}