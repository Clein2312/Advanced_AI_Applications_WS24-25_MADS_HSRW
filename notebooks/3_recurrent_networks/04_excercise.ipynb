{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools\n",
    "from pathlib import Path\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from src.models import rnn_models, train_model\n",
    "\n",
    "datadir = Path(\"../../data/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Iterators\n",
    "We will be using an interesting dataset. [link](https://tev.fbk.eu/resources/smartwatch)\n",
    "\n",
    "From the site:\n",
    "> The SmartWatch Gestures Dataset has been collected to evaluate several gesture recognition algorithms for interacting with mobile applications using arm gestures. Eight different users performed twenty repetitions of twenty different gestures, for a total of 3200 sequences. Each sequence contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatch™, as well as timestamps from the different clock sources available on an Android device. The smartwatch was worn on the user's right wrist. \n",
    "\n",
    "# 1.1 Iterator design pattern\n",
    "This is a nice opportunity to create our own custom iterator. First, let's look at the simplified design pattern for an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class BaseIterator:\n",
    "  def __init__(self, n: int):\n",
    "    self.n = n\n",
    "    self.data = [*range(self.n)]\n",
    "\n",
    "  def __iter__(self):\n",
    "    self.idx = -1\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.idx < len(self.data) - 1:\n",
    "      self.idx += 1\n",
    "      return self.data[self.idx]\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "myclass = BaseIterator(n=5)\n",
    "myiter = iter(myclass)\n",
    "\n",
    "for x in myiter:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `__iter__` returns `self` with the startsettings (`idx=-1`).\n",
    "Every time we call `__next__`, e.g. in a forloop or explicitly with `next()` \n",
    "\n",
    "If you keep calling `next()`, you will get a `StopIteration` error but the forloop will look out for that, so you should be good.\n",
    "\n",
    "## 1.2 Iterator implementation\n",
    "Now, we extend this pattern to our dataset. We will use what we have seen before with the images: we load the paths, and use these to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "class Datagenerator:\n",
    "    def __init__(self, paths: List[Path], batchsize:int ):\n",
    "        self.paths = paths\n",
    "        random.shuffle(self.paths)\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "        self.dataset = []\n",
    "        # If the files are too much to load at once,\n",
    "        # you could move this to __getitem__\n",
    "        # however, this is a small dataset and it is faster to load from memory\n",
    "        for file in tqdm(self.paths):\n",
    "            x = np.genfromtxt(file)[:, 3:]\n",
    "            x = torch.tensor(x).type(torch.float32)\n",
    "            y = int(file.parent.name) - 1\n",
    "            self.dataset.append((x, y))\n",
    "\n",
    "        self.size = len(self.dataset)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset) / self.batchsize)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        self.index_list = torch.randperm(self.size) \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X = []\n",
    "            Y = []\n",
    "            for _ in range(self.batchsize):\n",
    "                x, y = self[self.index_list[self.index]]\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                self.index += 1\n",
    "            # this makes all sequence of equal length by adding zeros\n",
    "            X_ = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "            return X_, torch.tensor(Y)\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with the pattern we used before, is that this pattern will stop after the last item is spit out.\n",
    "The generator from the first lesson will go on forever, due to the `while True` loop.\n",
    "\n",
    "## 1.3 Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../data/external/gestures-dataset/\")\n",
    "\n",
    "# get all paths with the .txt extension\n",
    "formats = [\".txt\"]\n",
    "paths = [path for path in data_tools.walk_dir(data_dir) if path.suffix in formats]\n",
    "# make a train-test split\n",
    "split = 0.8\n",
    "idx = int(len(paths) * split)\n",
    "trainpaths = paths[:idx]\n",
    "testpaths = paths[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2600/2600 [00:02<00:00, 1234.55it/s]\n",
      "100%|██████████| 651/651 [00:00<00:00, 1371.31it/s]\n"
     ]
    }
   ],
   "source": [
    "trainloader = Datagenerator(trainpaths, batchsize=32)\n",
    "testloader = Datagenerator(testpaths, batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 3]), 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = trainloader[0]\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the lenght mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trainloader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the shape?\n",
    "\n",
    "# 2 Excercises\n",
    "Lets test a basemodel, and try to improve upon that.\n",
    "\n",
    "Fill the gestures.gin file with relevant settings for `input_size`, `hidden_size`, `num_layers` and `horizon` (which, in our case, will be the number of classes...)\n",
    "\n",
    "As a rule of thumbs: start lower than you expect to need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures.gin\")\n",
    "model = rnn_models.BaseRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. What is the output shape you need? Remember, we are doing classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import metrics\n",
    "\n",
    "accuracy = metrics.Accuracy()\n",
    "accuracy(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy? What would you expect from blind guessing?\n",
    "\n",
    "Check shape of `y` and `yhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense to you? If you are unclear, go back to the classification problem with the MNIST, where we had 10 classes.\n",
    "\n",
    "We have a classification problem, so we need Cross Entropy Loss.\n",
    "Remember, [this has a softmax built in](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0288, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 14:55:27.701 | INFO     | src.data.data_tools:dir_add_timestamp:209 - Logging to ../../models/gestures/20220523-1455\n",
      "100%|██████████| 81/81 [00:01<00:00, 48.52it/s]\n",
      "2022-05-23 14:55:29.653 | INFO     | src.models.train_model:trainloop:156 - Epoch 0 train 2.4343 test 0.1240 metric ['0.1125']\n",
      "100%|██████████| 81/81 [00:01<00:00, 70.77it/s]\n",
      "2022-05-23 14:55:30.893 | INFO     | src.models.train_model:trainloop:156 - Epoch 1 train 2.2257 test 0.1232 metric ['0.0906']\n",
      "100%|██████████| 81/81 [00:01<00:00, 73.09it/s]\n",
      "2022-05-23 14:55:32.102 | INFO     | src.models.train_model:trainloop:156 - Epoch 2 train 2.2427 test 0.1152 metric ['0.1109']\n",
      "100%|██████████| 81/81 [00:00<00:00, 104.80it/s]\n",
      "2022-05-23 14:55:32.964 | INFO     | src.models.train_model:trainloop:156 - Epoch 3 train 2.1771 test 0.1162 metric ['0.1344']\n",
      "100%|██████████| 81/81 [00:00<00:00, 105.80it/s]\n",
      "2022-05-23 14:55:33.810 | INFO     | src.models.train_model:trainloop:156 - Epoch 4 train 2.1127 test 0.1158 metric ['0.1266']\n",
      "100%|██████████| 81/81 [00:01<00:00, 74.44it/s]\n",
      "2022-05-23 14:55:35.004 | INFO     | src.models.train_model:trainloop:156 - Epoch 5 train 2.0139 test 0.1074 metric ['0.1203']\n",
      "100%|██████████| 81/81 [00:01<00:00, 65.96it/s]\n",
      "2022-05-23 14:55:36.330 | INFO     | src.models.train_model:trainloop:156 - Epoch 6 train 2.2276 test 0.1020 metric ['0.1500']\n",
      "100%|██████████| 81/81 [00:00<00:00, 88.59it/s]\n",
      "2022-05-23 14:55:37.335 | INFO     | src.models.train_model:trainloop:156 - Epoch 7 train 2.0721 test 0.1112 metric ['0.1453']\n",
      "100%|██████████| 81/81 [00:00<00:00, 87.97it/s]\n",
      "2022-05-23 14:55:38.345 | INFO     | src.models.train_model:trainloop:156 - Epoch 8 train 2.1232 test 0.1067 metric ['0.1328']\n",
      "100%|██████████| 81/81 [00:00<00:00, 90.10it/s]\n",
      "2022-05-23 14:55:39.369 | INFO     | src.models.train_model:trainloop:156 - Epoch 9 train 2.1871 test 0.1018 metric ['0.1547']\n",
      "100%|██████████| 81/81 [00:00<00:00, 87.28it/s]\n",
      "2022-05-23 14:55:40.386 | INFO     | src.models.train_model:trainloop:156 - Epoch 10 train 1.9607 test 0.1129 metric ['0.1187']\n",
      "100%|██████████| 81/81 [00:00<00:00, 104.43it/s]\n",
      "2022-05-23 14:55:41.239 | INFO     | src.models.train_model:trainloop:156 - Epoch 11 train 1.9817 test 0.1268 metric ['0.1203']\n",
      "100%|██████████| 81/81 [00:01<00:00, 67.08it/s]\n",
      "2022-05-23 14:55:42.546 | INFO     | src.models.train_model:trainloop:156 - Epoch 12 train 2.3283 test 0.1082 metric ['0.1844']\n",
      "100%|██████████| 81/81 [00:00<00:00, 91.89it/s]\n",
      "2022-05-23 14:55:43.526 | INFO     | src.models.train_model:trainloop:156 - Epoch 13 train 2.2025 test 0.1095 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 97.59it/s]\n",
      "2022-05-23 14:55:44.449 | INFO     | src.models.train_model:trainloop:156 - Epoch 14 train 2.1044 test 0.1048 metric ['0.1391']\n",
      "100%|██████████| 81/81 [00:00<00:00, 104.54it/s]\n",
      "2022-05-23 14:55:45.311 | INFO     | src.models.train_model:trainloop:156 - Epoch 15 train 2.0168 test 0.1257 metric ['0.1688']\n",
      "100%|██████████| 81/81 [00:00<00:00, 101.02it/s]\n",
      "2022-05-23 14:55:46.202 | INFO     | src.models.train_model:trainloop:156 - Epoch 16 train 2.1451 test 0.1057 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 103.19it/s]\n",
      "2022-05-23 14:55:47.063 | INFO     | src.models.train_model:trainloop:156 - Epoch 17 train 1.9277 test 0.1046 metric ['0.1922']\n",
      "100%|██████████| 81/81 [00:00<00:00, 111.96it/s]\n",
      "2022-05-23 14:55:47.860 | INFO     | src.models.train_model:trainloop:156 - Epoch 18 train 2.5861 test 0.1097 metric ['0.0906']\n",
      "100%|██████████| 81/81 [00:01<00:00, 77.38it/s]\n",
      "2022-05-23 14:55:48.989 | INFO     | src.models.train_model:trainloop:156 - Epoch 19 train 2.3960 test 0.1073 metric ['0.1797']\n",
      "100%|██████████| 81/81 [00:00<00:00, 94.70it/s]\n",
      "2022-05-23 14:55:49.921 | INFO     | src.models.train_model:trainloop:156 - Epoch 20 train 2.1126 test 0.1185 metric ['0.1375']\n",
      "100%|██████████| 81/81 [00:00<00:00, 91.77it/s]\n",
      "2022-05-23 14:55:50.888 | INFO     | src.models.train_model:trainloop:156 - Epoch 21 train 2.2291 test 0.1006 metric ['0.1531']\n",
      "100%|██████████| 81/81 [00:00<00:00, 102.16it/s]\n",
      "2022-05-23 14:55:51.766 | INFO     | src.models.train_model:trainloop:156 - Epoch 22 train 1.8533 test 0.1072 metric ['0.1891']\n",
      "100%|██████████| 81/81 [00:00<00:00, 96.37it/s]\n",
      "2022-05-23 14:55:52.692 | INFO     | src.models.train_model:trainloop:156 - Epoch 23 train 2.1722 test 0.1167 metric ['0.1562']\n",
      "100%|██████████| 81/81 [00:00<00:00, 89.67it/s] \n",
      "2022-05-23 14:55:53.675 | INFO     | src.models.train_model:trainloop:156 - Epoch 24 train 2.3220 test 0.1083 metric ['0.1516']\n",
      "100%|██████████| 81/81 [00:00<00:00, 83.50it/s]\n",
      "2022-05-23 14:55:54.773 | INFO     | src.models.train_model:trainloop:156 - Epoch 25 train 1.8481 test 0.1145 metric ['0.1531']\n",
      "100%|██████████| 81/81 [00:00<00:00, 88.35it/s]\n",
      "2022-05-23 14:55:55.787 | INFO     | src.models.train_model:trainloop:156 - Epoch 26 train 1.9393 test 0.1043 metric ['0.1781']\n",
      "100%|██████████| 81/81 [00:00<00:00, 104.75it/s]\n",
      "2022-05-23 14:55:56.637 | INFO     | src.models.train_model:trainloop:156 - Epoch 27 train 2.2111 test 0.1048 metric ['0.2000']\n",
      "100%|██████████| 81/81 [00:00<00:00, 94.41it/s]\n",
      "2022-05-23 14:55:57.610 | INFO     | src.models.train_model:trainloop:156 - Epoch 28 train 2.0293 test 0.1088 metric ['0.1156']\n",
      "100%|██████████| 81/81 [00:00<00:00, 87.82it/s] \n",
      "2022-05-23 14:55:58.613 | INFO     | src.models.train_model:trainloop:156 - Epoch 29 train 1.9625 test 0.1334 metric ['0.1328']\n",
      "100%|██████████| 81/81 [00:00<00:00, 98.13it/s]\n",
      "2022-05-23 14:55:59.545 | INFO     | src.models.train_model:trainloop:156 - Epoch 30 train 2.1819 test 0.1117 metric ['0.1437']\n",
      "100%|██████████| 81/81 [00:00<00:00, 85.91it/s]\n",
      "2022-05-23 14:56:00.579 | INFO     | src.models.train_model:trainloop:156 - Epoch 31 train 2.3337 test 0.1103 metric ['0.1578']\n",
      "100%|██████████| 81/81 [00:00<00:00, 104.81it/s]\n",
      "2022-05-23 14:56:01.429 | INFO     | src.models.train_model:trainloop:156 - Epoch 32 train 2.2532 test 0.1171 metric ['0.1141']\n",
      "100%|██████████| 81/81 [00:00<00:00, 87.81it/s]\n",
      "2022-05-23 14:56:02.445 | INFO     | src.models.train_model:trainloop:156 - Epoch 33 train 2.0809 test 0.1039 metric ['0.1562']\n",
      "100%|██████████| 81/81 [00:00<00:00, 95.82it/s] \n",
      "2022-05-23 14:56:03.361 | INFO     | src.models.train_model:trainloop:156 - Epoch 34 train 1.9447 test 0.1023 metric ['0.1453']\n",
      "100%|██████████| 81/81 [00:00<00:00, 102.89it/s]\n",
      "2022-05-23 14:56:04.242 | INFO     | src.models.train_model:trainloop:156 - Epoch 35 train 2.1903 test 0.1129 metric ['0.1703']\n",
      "100%|██████████| 81/81 [00:00<00:00, 89.49it/s]\n",
      "2022-05-23 14:56:05.238 | INFO     | src.models.train_model:trainloop:156 - Epoch 36 train 2.0845 test 0.1144 metric ['0.1375']\n",
      "100%|██████████| 81/81 [00:00<00:00, 108.91it/s]\n",
      "2022-05-23 14:56:06.067 | INFO     | src.models.train_model:trainloop:156 - Epoch 37 train 2.1140 test 0.1011 metric ['0.1672']\n",
      "100%|██████████| 81/81 [00:00<00:00, 88.33it/s]\n",
      "2022-05-23 14:56:07.075 | INFO     | src.models.train_model:trainloop:156 - Epoch 38 train 2.2262 test 0.1081 metric ['0.1391']\n",
      "100%|██████████| 81/81 [00:00<00:00, 82.71it/s]\n",
      "2022-05-23 14:56:08.128 | INFO     | src.models.train_model:trainloop:156 - Epoch 39 train 1.7431 test 0.1032 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 109.41it/s]\n",
      "2022-05-23 14:56:08.940 | INFO     | src.models.train_model:trainloop:156 - Epoch 40 train 1.9118 test 0.1010 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 108.53it/s]\n",
      "2022-05-23 14:56:09.774 | INFO     | src.models.train_model:trainloop:156 - Epoch 41 train 2.2366 test 0.1148 metric ['0.1187']\n",
      "100%|██████████| 81/81 [00:00<00:00, 109.73it/s]\n",
      "2022-05-23 14:56:10.591 | INFO     | src.models.train_model:trainloop:156 - Epoch 42 train 1.9543 test 0.1255 metric ['0.1531']\n",
      "100%|██████████| 81/81 [00:00<00:00, 115.90it/s]\n",
      "2022-05-23 14:56:11.383 | INFO     | src.models.train_model:trainloop:156 - Epoch 43 train 1.7838 test 0.1036 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 93.64it/s]\n",
      "2022-05-23 14:56:12.323 | INFO     | src.models.train_model:trainloop:156 - Epoch 44 train 1.8454 test 0.1160 metric ['0.1609']\n",
      "100%|██████████| 81/81 [00:00<00:00, 115.36it/s]\n",
      "2022-05-23 14:56:13.115 | INFO     | src.models.train_model:trainloop:156 - Epoch 45 train 1.9206 test 0.1293 metric ['0.1453']\n",
      "100%|██████████| 81/81 [00:00<00:00, 102.13it/s]\n",
      "2022-05-23 14:56:13.993 | INFO     | src.models.train_model:trainloop:156 - Epoch 46 train 1.9997 test 0.1162 metric ['0.1422']\n",
      "100%|██████████| 81/81 [00:00<00:00, 107.16it/s]\n",
      "2022-05-23 14:56:14.829 | INFO     | src.models.train_model:trainloop:156 - Epoch 47 train 1.8399 test 0.0993 metric ['0.1688']\n",
      "100%|██████████| 81/81 [00:00<00:00, 111.79it/s]\n",
      "2022-05-23 14:56:15.629 | INFO     | src.models.train_model:trainloop:156 - Epoch 48 train 1.7837 test 0.1092 metric ['0.0906']\n",
      "100%|██████████| 81/81 [00:00<00:00, 115.36it/s]\n",
      "2022-05-23 14:56:16.407 | INFO     | src.models.train_model:trainloop:156 - Epoch 49 train 2.1317 test 0.1114 metric ['0.1594']\n",
      "100%|██████████| 50/50 [00:48<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "gin.parse_config_file(\"gestures.gin\")\n",
    "\n",
    "log_dir = Path(\"../../models/gestures/\")\n",
    "\n",
    "model = rnn_models.BaseRNN()\n",
    "\n",
    "model = train_model.trainloop(\n",
    "    model=model,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=trainloader,\n",
    "    test_dataloader=testloader,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=len(trainloader),\n",
    "    eval_steps=len(testloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercises:\n",
    "\n",
    "- improve the RNN model\n",
    "- test different things. What works? What does not?\n",
    "- make a second model, where you test a GRU layer. Create an additional `.gin` file for this second model. \n",
    "- Try to add Conv1D layers.\n",
    "\n",
    "You should be able to get above 90% accuracy with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures_gru.gin\")\n",
    "\n",
    "log_dir = Path(\"../../models/gestures/\")\n",
    "\n",
    "model = rnn_models.GRUmodel()\n",
    "\n",
    "model = rnn_models.trainloop(\n",
    "    model=model,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=trainloader,\n",
    "    test_dataloader=testloader,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=len(trainloader),\n",
    "    eval_steps=len(testloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
