{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools\n",
    "from pathlib import Path\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from src.models import rnn_models, train_model\n",
    "from torch import optim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Iterators\n",
    "We will be using an interesting dataset. [link](https://tev.fbk.eu/resources/smartwatch)\n",
    "\n",
    "From the site:\n",
    "> The SmartWatch Gestures Dataset has been collected to evaluate several gesture recognition algorithms for interacting with mobile applications using arm gestures. Eight different users performed twenty repetitions of twenty different gestures, for a total of 3200 sequences. Each sequence contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatch™, as well as timestamps from the different clock sources available on an Android device. The smartwatch was worn on the user's right wrist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from src.settings import gesturesdatasetsettings\n",
    "from src.data.make_dataset import DatasetFactoryProvider, DatasetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesturesdatasetfactory = DatasetFactoryProvider.get_factory(DatasetType.GESTURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:18:32.740 | INFO     | src.data.make_dataset:download_data:99 - Dataset already exists at /workspaces/ML22/data/raw/gestures\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 2600/2600 [00:06<00:00, 409.10it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 651/651 [00:01<00:00, 393.04it/s]\n"
     ]
    }
   ],
   "source": [
    "streamers = gesturesdatasetfactory.create_datastreamer(batchsize=32)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 3]),\n",
       " tensor([ 4,  0, 17, 10,  5,  8,  2,  8,  7, 19,  0,  0,  3, 19,  5,  9,  0, 13,\n",
       "         11, 12, 11, 17,  3,  4,  8,  6, 19, 17,  0,  5,  9,  1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the shape?\n",
    "What does it mean that the shapes are sometimes (32, 27, 3), but a second time might look like (32, 30, 3)? In other words, the second (or first, if you insist on starting at 0) dimension changes. Why is that? How does the model handle this? Do you think this is already padded, or still has to be padded?\n",
    "\n",
    "\n",
    "# 2 Excercises\n",
    "Lets test a basemodel, and try to improve upon that.\n",
    "\n",
    "Fill the gestures.gin file with relevant settings for `input_size`, `hidden_size`, `num_layers` and `horizon` (which, in our case, will be the number of classes...)\n",
    "\n",
    "As a rule of thumbs: start lower than you expect to need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: gestures\n",
       "train_steps: 81\n",
       "valid_steps: 20\n",
       "reporttypes: [<ReportTypes.GIN: 1>, <ReportTypes.TENSORBOARD: 2>, <ReportTypes.MLFLOW: 3>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings, ReportTypes\n",
    "from src.models.metrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=Path(\"gestures\"),\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.GIN, ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    "    earlystop_kwargs=None\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures.gin\")\n",
    "model = rnn_models.BaseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 3, 'hidden_size': 128, 'num_layers': 3, 'horizon': 20}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.get_bindings(\"BaseRNN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. What is the output shape you need? Remember, we are doing classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy? What would you expect from blind guessing?\n",
    "\n",
    "Check shape of `y` and `yhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at the output of yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0349,  0.1734, -0.0380, -0.3100, -0.1482,  0.2014, -0.0265,  0.0360,\n",
       "        -0.1577, -0.0623, -0.0639, -0.1680,  0.1687, -0.0143,  0.0157, -0.0162,\n",
       "        -0.1668,  0.0869,  0.0069, -0.2994], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense to you? If you are unclear, go back to the classification problem with the MNIST, where we had 10 classes.\n",
    "\n",
    "We have a classification problem, so we need Cross Entropy Loss.\n",
    "Remember, [this has a softmax built in](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9984, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:19:55.950 | INFO     | src.data.data_tools:dir_add_timestamp:146 - Logging to gestures/20230602-1519\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:06<00:00, 13.31it/s]\n",
      "2023-06-02 15:20:02.734 | INFO     | src.models.train_model:report:210 - Epoch 0 train 2.6137 test 2.3226 metric ['0.1625']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 13.72it/s]\n",
      "2023-06-02 15:20:09.331 | INFO     | src.models.train_model:report:210 - Epoch 1 train 2.1959 test 2.1054 metric ['0.2609']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 13.56it/s]\n",
      "2023-06-02 15:20:16.040 | INFO     | src.models.train_model:report:210 - Epoch 2 train 1.8563 test 1.7374 metric ['0.3406']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 14.85it/s]\n",
      "2023-06-02 15:20:22.205 | INFO     | src.models.train_model:report:210 - Epoch 3 train 1.4457 test 1.2887 metric ['0.5016']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 13.88it/s]\n",
      "2023-06-02 15:20:28.673 | INFO     | src.models.train_model:report:210 - Epoch 4 train 1.1159 test 1.0135 metric ['0.5781']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:04<00:00, 17.00it/s]\n",
      "2023-06-02 15:20:34.025 | INFO     | src.models.train_model:report:210 - Epoch 5 train 0.9451 test 0.8223 metric ['0.6734']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:06<00:00, 13.35it/s]\n",
      "2023-06-02 15:20:40.700 | INFO     | src.models.train_model:report:210 - Epoch 6 train 0.7444 test 0.7328 metric ['0.6984']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:06<00:00, 12.74it/s]\n",
      "2023-06-02 15:20:47.960 | INFO     | src.models.train_model:report:210 - Epoch 7 train 0.5855 test 0.5521 metric ['0.7984']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 15.65it/s]\n",
      "2023-06-02 15:20:53.742 | INFO     | src.models.train_model:report:210 - Epoch 8 train 0.4682 test 0.4093 metric ['0.8438']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:05<00:00, 15.83it/s]\n",
      "2023-06-02 15:20:59.545 | INFO     | src.models.train_model:report:210 - Epoch 9 train 0.3929 test 0.3293 metric ['0.8828']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [01:03<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"../../models/gestures/\").resolve()\n",
    "gin.parse_config_file(\"gestures.gin\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"model\", \"GRUmodel\")\n",
    "    mlflow.set_tag(\"dev\", \"raoul\")\n",
    "    mlflow.log_params(gin.get_bindings(\"BaseRNN\")[\"config\"])\n",
    "    mlflow.log_params(gin.get_bindings(\"trainloop\"))\n",
    "\n",
    "    model = rnn_models.BaseRNN()\n",
    "    trainer = train_model.Trainer(\n",
    "        model=model, \n",
    "        settings=settings, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim.Adam, \n",
    "        traindataloader=trainstreamer, \n",
    "        validdataloader=validstreamer, \n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "    trainer.loop()\n",
    "\n",
    "    tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    modelpath = modeldir / (tag + \"model.pt\")\n",
    "    torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures_gru.gin\")\n",
    "model = rnn_models.GRUmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercises:\n",
    "\n",
    "- improve the RNN model\n",
    "- test different things. What works? What does not?\n",
    "- experiment with either GRU or LSTM layers, create your own models + ginfiles. \n",
    "- experiment with adding Conv1D layers.\n",
    "\n",
    "You should be able to get above 90% accuracy with the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
