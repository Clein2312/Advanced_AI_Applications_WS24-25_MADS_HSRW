{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools\n",
    "from pathlib import Path\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from src.models import rnn_models\n",
    "\n",
    "datadir = Path(\"../../data/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Iterators\n",
    "We will be using an interesting dataset. [link](https://tev.fbk.eu/resources/smartwatch)\n",
    "\n",
    "From the site:\n",
    "> The SmartWatch Gestures Dataset has been collected to evaluate several gesture recognition algorithms for interacting with mobile applications using arm gestures. Eight different users performed twenty repetitions of twenty different gestures, for a total of 3200 sequences. Each sequence contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatch™, as well as timestamps from the different clock sources available on an Android device. The smartwatch was worn on the user's right wrist. \n",
    "\n",
    "# 1.1 Iterator design pattern\n",
    "This is a nice opportunity to create our own custom iterator. First, let's look at the simplified design pattern for an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class BaseIterator:\n",
    "  def __init__(self, n: int):\n",
    "    self.n = n\n",
    "    self.data = [*range(self.n)]\n",
    "\n",
    "  def __iter__(self):\n",
    "    self.idx = -1\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.idx < len(self.data) - 1:\n",
    "      self.idx += 1\n",
    "      return self.data[self.idx]\n",
    "    else:\n",
    "      raise StopIteration\n",
    "\n",
    "myclass = BaseIterator(n=5)\n",
    "myiter = iter(myclass)\n",
    "\n",
    "for x in myiter:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `__iter__` returns `self` with the startsettings (`idx=-1`).\n",
    "Every time we call `__next__`, e.g. in a forloop or explicitly with `next()` \n",
    "\n",
    "If you keep calling `next()`, you will get a `StopIteration` error but the forloop will look out for that, so you should be good.\n",
    "\n",
    "## 1.2 Iterator implementation\n",
    "Now, we extend this pattern to our dataset. We will use what we have seen before with the images: we load the paths, and use these to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "class Datagenerator:\n",
    "    def __init__(self, paths: List[Path], batchsize:int ):\n",
    "        self.paths = paths\n",
    "        random.shuffle(self.paths)\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "        self.dataset = []\n",
    "        for file in tqdm(self.paths):\n",
    "            x = np.genfromtxt(file)[:, 3:]\n",
    "            x = torch.tensor(x).type(torch.float32)\n",
    "            y = int(file.parent.name) - 1\n",
    "            self.dataset.append((x, y))\n",
    "\n",
    "        self.size = len(self.dataset)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset) / self.batchsize)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        self.index_list = torch.randperm(self.size) \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X = []\n",
    "            Y = []\n",
    "            for _ in range(self.batchsize):\n",
    "                x, y = self[self.index_list[self.index]]\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                self.index += 1\n",
    "            # this makes all sequence of equal length by adding zeros\n",
    "            X_ = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "            return X_, torch.tensor(Y)\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with the pattern we used before, is that this pattern will stop after the last item is spit out.\n",
    "The generator from the first lesson will go on forever, due to the `while True` loop.\n",
    "\n",
    "## 1.3 Train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../data/external/gestures-dataset/\")\n",
    "\n",
    "# get all paths with the .txt extension\n",
    "formats = [\".txt\"]\n",
    "paths = [path for path in data_tools.walk_dir(data_dir) if path.suffix in formats]\n",
    "# make a train-test split\n",
    "split = 0.8\n",
    "idx = int(len(paths) * split)\n",
    "trainpaths = paths[:idx]\n",
    "testpaths = paths[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2600/2600 [00:01<00:00, 2350.39it/s]\n",
      "100%|██████████| 651/651 [00:00<00:00, 2015.72it/s]\n"
     ]
    }
   ],
   "source": [
    "trainloader = Datagenerator(trainpaths, batchsize=32)\n",
    "testloader = Datagenerator(testpaths, batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 3]), 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = trainloader[0]\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the lenght mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trainloader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the shape?\n",
    "\n",
    "# 2 Excercises\n",
    "Lets test a basemodel, and try to improve upon that.\n",
    "\n",
    "Fill the gestures.gin file with relevant settings for `input_size`, `hidden_size`, `num_layers` and `horizon` (which, in our case, will be the number of classes...)\n",
    "\n",
    "As a rule of thumbs: start lower than you expect to need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures.gin\")\n",
    "model = rnn_models.BaseRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. What is the output shape you need? Remember, we are doing classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import metrics\n",
    "\n",
    "accuracy = metrics.Accuracy()\n",
    "accuracy(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy? What would you expect from blind guessing?\n",
    "\n",
    "Check shape of `y` and `yhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense to you? If you are unclear, go back to the classification problem with the MNIST, where we had 10 classes.\n",
    "\n",
    "We have a classification problem, so we need Cross Entropy Loss.\n",
    "Remember, [this has a softmax built in](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0138, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 09:00:52.545 | INFO     | src.data.data_tools:clean_dir:167 - Clean out ../../models/gestures/20220517-0900\n",
      "100%|██████████| 50/50 [00:35<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "gin.parse_config_file(\"gestures.gin\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "log_dir = Path(\"../../models/gestures/\" + timestamp)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "\n",
    "\n",
    "model = rnn_models.BaseRNN()\n",
    "\n",
    "model = rnn_models.trainloop(\n",
    "    model=model,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=trainloader,\n",
    "    test_dataloader=testloader,\n",
    "    log_dir=log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercises:\n",
    "\n",
    "- improve the RNN model\n",
    "- test different things. What works? What does not?\n",
    "- make a second model, where you test a GRU layer. Create an additional `.gin` file for this second model. \n",
    "- Try to add Conv1D layers.\n",
    "\n",
    "You should be able to get above 90% accuracy with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures_gru.gin\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "log_dir = Path(\"../../models/gestures/\" + timestamp)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "\n",
    "\n",
    "model = rnn_models.GRUmodel()\n",
    "\n",
    "model = rnn_models.trainloop(\n",
    "    model=model,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=trainloader,\n",
    "    test_dataloader=testloader,\n",
    "    log_dir=log_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
