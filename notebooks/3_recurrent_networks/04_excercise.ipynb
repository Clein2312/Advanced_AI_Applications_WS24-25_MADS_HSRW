{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/rgrouls/code/ML22 to sys.path, this is necessary to import from src\n",
      "['/Users/rgrouls/code/ML22', '/Users/rgrouls/code/ML22/notebooks/3_recurrent_networks', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python39.zip', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9/lib-dynload', '', '/Users/rgrouls/Library/Caches/pypoetry/virtualenvs/deep-learning-HUU8cknU-py3.9/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "srcdir = Path(\"../..\").resolve()\n",
    "print(f\"Adding {srcdir} to sys.path, this is necessary to import from src\")\n",
    "sys.path.insert(0, str(srcdir))\n",
    "print(sys.path)\n",
    "\n",
    "from src.data import data_tools\n",
    "from pathlib import Path\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from src.models import rnn_models, train_model\n",
    "from torch import optim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Iterators\n",
    "We will be using an interesting dataset. [link](https://tev.fbk.eu/resources/smartwatch)\n",
    "\n",
    "From the site:\n",
    "> The SmartWatch Gestures Dataset has been collected to evaluate several gesture recognition algorithms for interacting with mobile applications using arm gestures. Eight different users performed twenty repetitions of twenty different gestures, for a total of 3200 sequences. Each sequence contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatch™, as well as timestamps from the different clock sources available on an Android device. The smartwatch was worn on the user's right wrist. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 11:32:11.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset already exists at /Users/rgrouls/.cache/mads_datasets/gestures\u001b[0m\n",
      "\u001b[32m2023-06-06 11:32:11.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mDigest of downloaded /Users/rgrouls/.cache/mads_datasets/gestures/gestures-dataset.zip matches expected digest\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 2600/2600 [00:02<00:00, 901.71it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 651/651 [00:00<00:00, 921.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "gesturesdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.GESTURES)\n",
    "streamers = gesturesdatasetfactory.create_datastreamer(batchsize=32)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 51, 3]),\n",
       " tensor([17,  0,  7,  9,  5,  5,  9, 13, 18, 14,  7, 14, 17,  5, 11,  6, 19,  5,\n",
       "          6,  4, 19,  0, 12, 14,  4, 19, 17, 17,  2,  0,  6, 10]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the shape?\n",
    "What does it mean that the shapes are sometimes (32, 27, 3), but a second time might look like (32, 30, 3)? In other words, the second (or first, if you insist on starting at 0) dimension changes. Why is that? How does the model handle this? Do you think this is already padded, or still has to be padded?\n",
    "\n",
    "\n",
    "# 2 Excercises\n",
    "Lets test a basemodel, and try to improve upon that.\n",
    "\n",
    "Fill the gestures.gin file with relevant settings for `input_size`, `hidden_size`, `num_layers` and `horizon` (which, in our case, will be the number of classes...)\n",
    "\n",
    "As a rule of thumbs: start lower than you expect to need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: gestures\n",
       "train_steps: 81\n",
       "valid_steps: 20\n",
       "reporttypes: [<ReportTypes.GIN: 1>, <ReportTypes.TENSORBOARD: 2>, <ReportTypes.MLFLOW: 3>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings, ReportTypes\n",
    "from src.models.metrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=Path(\"gestures\"),\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.GIN, ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    "    earlystop_kwargs=None\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"gestures.gin\")\n",
    "model = rnn_models.BaseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 3, 'hidden_size': 128, 'num_layers': 3, 'horizon': 20}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.get_bindings(\"BaseRNN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model. What is the output shape you need? Remember, we are doing classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(x)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy? What would you expect from blind guessing?\n",
    "\n",
    "Check shape of `y` and `yhat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at the output of yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0121, -0.0644, -0.0528, -0.0295, -0.0008,  0.0250,  0.0220, -0.1761,\n",
       "        -0.0566, -0.0998, -0.0546,  0.0126, -0.1668, -0.0077, -0.0334,  0.0628,\n",
       "        -0.0624, -0.0496, -0.1477, -0.0023], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense to you? If you are unclear, go back to the classification problem with the MNIST, where we had 10 classes.\n",
    "\n",
    "We have a classification problem, so we need Cross Entropy Loss.\n",
    "Remember, [this has a softmax built in](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9951, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 3, 'hidden_size': 128, 'num_layers': 3, 'horizon': 20}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.get_bindings(\"BaseRNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 11:34:05.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.data_tools\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mLogging to gestures/20230606-1134\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 53.30it/s]\n",
      "\u001b[32m2023-06-06 11:34:07.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 0 train 2.6675 test 2.4786 metric ['0.1203']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 61.28it/s]\n",
      "\u001b[32m2023-06-06 11:34:08.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 1 train 2.4253 test 2.4161 metric ['0.1250']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 55.25it/s]\n",
      "\u001b[32m2023-06-06 11:34:10.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 2 train 2.3768 test 2.4860 metric ['0.1391']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 60.56it/s]\n",
      "\u001b[32m2023-06-06 11:34:11.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 3 train 2.3681 test 2.3243 metric ['0.1594']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 57.42it/s]\n",
      "\u001b[32m2023-06-06 11:34:13.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 4 train 2.3046 test 2.2737 metric ['0.1609']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 50.69it/s]\n",
      "\u001b[32m2023-06-06 11:34:15.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 5 train 2.2555 test 2.2596 metric ['0.1500']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 54.76it/s]\n",
      "\u001b[32m2023-06-06 11:34:16.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 6 train 2.6214 test 3.0178 metric ['0.0813']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 57.38it/s]\n",
      "\u001b[32m2023-06-06 11:34:18.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 7 train 3.0132 test 3.0307 metric ['0.0500']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 40.46it/s]\n",
      "\u001b[32m2023-06-06 11:34:20.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 8 train 3.0101 test 3.0106 metric ['0.0469']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 28.56it/s]\n",
      "\u001b[32m2023-06-06 11:34:23.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mEpoch 9 train 3.0286 test 3.0309 metric ['0.0422']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [00:17<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"../../models/gestures/\").resolve()\n",
    "gin.parse_config_file(\"gestures.gin\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"model\", \"GRUmodel\")\n",
    "    mlflow.set_tag(\"dev\", \"raoul\")\n",
    "    mlflow.log_params(gin.get_bindings(\"BaseRNN\"))\n",
    "\n",
    "    model = rnn_models.BaseRNN()\n",
    "    trainer = train_model.Trainer(\n",
    "        model=model, \n",
    "        settings=settings, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optim.Adam, \n",
    "        traindataloader=trainstreamer, \n",
    "        validdataloader=validstreamer, \n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "    trainer.loop()\n",
    "\n",
    "    tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    modelpath = modeldir / (tag + \"model.pt\")\n",
    "    torch.save(model, modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to update the code above with the following two commands.\n",
    "    \n",
    "```python\n",
    "gin.parse_config_file('gestures.gin')\n",
    "model = rnn_model.RNNModel()\n",
    "```\n",
    "\n",
    "To discern between the changes, also modify the tag mlflow.set_tag(\"model\", \"new-tag-here\") where you add\n",
    "a new tag of your choice. This way you can keep the models apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercises:\n",
    "\n",
    "- improve the RNN model\n",
    "- test different things. What works? What does not?\n",
    "- experiment with either GRU or LSTM layers, create your own models + ginfiles. \n",
    "- experiment with adding Conv1D layers.\n",
    "\n",
    "You should be able to get above 90% accuracy with the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
