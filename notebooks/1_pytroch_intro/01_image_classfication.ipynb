{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "## 3. Train the model\n",
    "We have seen in the `linearmodel` notebook how we can create a neural network architecture \n",
    "from scratch. However, we did not implement this with `jax`. While `jax` is created to build\n",
    "neural networks from scratch, it is really low-level. We will often be comfortable with having \n",
    "a bit more high-level in building the models. \n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data.__getitem__(0)\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is a tuple of image (tensor) and label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention. Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12345ed30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:11:41.284 | INFO     | src.models.train_model:trainloop:39 - Epoch 0 train 0.0056 | test 0.0059\n",
      "2022-04-05 09:11:53.036 | INFO     | src.models.train_model:trainloop:39 - Epoch 1 train 0.0050 | test 0.0059\n",
      "2022-04-05 09:12:05.490 | INFO     | src.models.train_model:trainloop:39 - Epoch 2 train 0.0047 | test 0.0057\n",
      "2022-04-05 09:12:17.497 | INFO     | src.models.train_model:trainloop:39 - Epoch 3 train 0.0044 | test 0.0054\n",
      "2022-04-05 09:12:29.599 | INFO     | src.models.train_model:trainloop:39 - Epoch 4 train 0.0041 | test 0.0057\n",
      "2022-04-05 09:12:41.827 | INFO     | src.models.train_model:trainloop:39 - Epoch 5 train 0.0039 | test 0.0052\n",
      "2022-04-05 09:12:54.046 | INFO     | src.models.train_model:trainloop:39 - Epoch 6 train 0.0037 | test 0.0055\n",
      "2022-04-05 09:13:07.660 | INFO     | src.models.train_model:trainloop:39 - Epoch 7 train 0.0035 | test 0.0051\n",
      "2022-04-05 09:13:20.888 | INFO     | src.models.train_model:trainloop:39 - Epoch 8 train 0.0034 | test 0.0050\n",
      "2022-04-05 09:13:35.535 | INFO     | src.models.train_model:trainloop:39 - Epoch 9 train 0.0032 | test 0.0053\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from src.models import train_model\n",
    "model = train_model.trainloop(\n",
    "    epochs=10,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.875"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "accuracy.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the trained model\n",
    "modelpath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# remove the data\n",
    "data = (Path(datadir) / \"FashionMNIST\")\n",
    "if data.exists():\n",
    "    shutil.rmtree(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f991136b32d0e931878e2d826f60fb70ad3d0a23fd6e1a56ea114087d779837"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deep-learning-uo9RXddf-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
