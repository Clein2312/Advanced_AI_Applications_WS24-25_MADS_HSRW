{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofar, we have seen how to make calculations with torch, and how to build a datagenerator. \n",
    "\n",
    "So, in theory we have enough knowledge to deliver the data in batches to our machine learning model to perform calculations on the data.\n",
    "\n",
    "But how to adjust the weights? How does the model learn which weights should be adjusted in which direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([6.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with two tensors `a` and `b`.\n",
    "\n",
    "We create a new tensor $Q$ with a calculation:\n",
    "$$\n",
    "Q = w x + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 10., 12.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = w * x + b\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a certain outcome. But how do we know if this is correct? For that, we need \n",
    "\n",
    "- some sort of ground truth.\n",
    "- a way to calculate the error\n",
    "\n",
    "A common way to calculate the error is the Mean Square Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y: torch.Tensor, yhat: torch.Tensor) -> torch.Tensor:\n",
    "    return ((y - yhat)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.,  9., 13.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 4 * x + 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(y, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need the gradients of the error with respect to the parameters. This means we want:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could calculate the derivatives by hand, which is tedious, especially if you have many nested calculations. But because our two parameters `w` and `b` where marked with `requires_grad=True`, the gradient was tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3333]), tensor([2.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we would adjust the weights by a certain factor, the learning rate. Typically this is set to `1e-3` , but it can be as big as `1e-1` and as small as `1e-5`. \n",
    "\n",
    "It can even vary during training: you start with `1e-1`, and if the improvement of the learning slows down you decrease the learning rate with a certain factor, e.g. to `1e-2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.8667], grad_fn=<SubBackward0>),\n",
       " tensor([5.8000], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "w = w - learning_rate * w.grad \n",
    "b = b - learning_rate * b.grad\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the adjustment of the weights, the training continues:\n",
    "\n",
    "- make a prediction\n",
    "- calculate the loss\n",
    "- calculate the gradients\n",
    "- adjust the weights with respect to the error with a certain rate\n",
    "\n",
    "And this is how the model learns!\n",
    "\n",
    "# From zero to deep learning\n",
    "What would this look like with a real model?\n",
    "Let's download a preptrained resnet18 model, see the [docs](https://pytorch.org/vision/main/models.html) for more models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "torch.hub.set_dir(\"../../models/\")\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show how the learning is done from start to end. We will encounter:\n",
    "\n",
    "- the prediction\n",
    "- the loss\n",
    "- the optimizer\n",
    "\n",
    "While there is a lot more to say about picking loss and optimizers, the defaults often work pretty decent. So, use MSE for regression problems, and use either an `SGD` or `Adam` as an optimization algorithm. That is enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a dummy image and dummy labels for prediction. Note that the Width, Height and RGB Channels of images are often in the order (W, H, C), but torch uses the (C, W, H) convention.\n",
    "\n",
    "We can make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model(data)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet18 is trained on 1000 classes, so the output are thousand numbers: one for every class it considers.\n",
    "\n",
    "We can calculate a loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(yhat, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the resnet has many many weights, and we also have different ways to optimize the weights, we need to pick an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "For now, we will pick the SGD from the [list of optimizer available](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.step()`, all gradients are calculated and all trainable parameters are adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can train another batch, and adjust weights, etc etc."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f991136b32d0e931878e2d826f60fb70ad3d0a23fd6e1a56ea114087d779837"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deep-learning-uo9RXddf-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
