{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:10:25.215940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-19 21:10:25.216000: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feba3eb2e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"../../models/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:10:36.165 | INFO     | src.data.data_tools:dir_add_timestamp:208 - Logging to ../../models/test/20220519-2110\n",
      "2022-05-19 21:10:36.167 | INFO     | src.data.data_tools:dir_add_timestamp:208 - Logging to ../../models/test/20220519-2110/20220519-2110\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.47it/s]\n",
      "2022-05-19 21:10:38.535 | INFO     | src.models.train_model:trainloop:148 - Epoch 0 train 0.6188 test 0.0152 metric ['0.7437']\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.54it/s]\n",
      "2022-05-19 21:10:40.085 | INFO     | src.models.train_model:trainloop:148 - Epoch 1 train 0.7345 test 0.0101 metric ['0.7794']\n",
      "100%|██████████| 50/50 [00:00<00:00, 60.35it/s]\n",
      "2022-05-19 21:10:41.311 | INFO     | src.models.train_model:trainloop:148 - Epoch 2 train 0.5565 test 0.0185 metric ['0.7925']\n",
      "100%|██████████| 50/50 [00:00<00:00, 63.97it/s]\n",
      "2022-05-19 21:10:42.478 | INFO     | src.models.train_model:trainloop:148 - Epoch 3 train 0.5068 test 0.0080 metric ['0.7984']\n",
      "100%|██████████| 50/50 [00:00<00:00, 60.88it/s]\n",
      "2022-05-19 21:10:43.695 | INFO     | src.models.train_model:trainloop:148 - Epoch 4 train 0.4539 test 0.0112 metric ['0.8194']\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model.trainloop(\n",
    "    epochs=5,\n",
    "    model=model,\n",
    "    optimizer=optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=50,\n",
    "    eval_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.1875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "acc.item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:10:44.735 | INFO     | src.data.data_tools:clean_dir:176 - Clean out ../../models/test\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b8f312320cd240106b9ea4d318428341e8727b3c7d5fc1f73cfe4a3d9868ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deep-learning-E14Cnx23-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
