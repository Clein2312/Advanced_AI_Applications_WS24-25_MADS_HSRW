{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbc2b23ec70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 256]         131,328\n",
      "              ReLU-5                  [-1, 256]               0\n",
      "            Linear-6                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.04\n",
      "Estimated Total Size (MB): 2.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=Path(\"../../models/test\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /workspaces/ML22/models/test\n",
       "train_steps: 938\n",
       "valid_steps: 157\n",
       "tunewriter: ['tensorboard']\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.1, 'patience': 10}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train_dataloader),\n",
    "    valid_steps=len(test_dataloader),\n",
    "    tunewrite=[\"tensorboard\"],\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 10:25:21.818 | INFO     | src.data.data_tools:dir_add_timestamp:137 - Logging to /workspaces/ML22/models/test/20230523-1025\n",
      "2023-05-23 10:25:21.830 | INFO     | src.models.train_model:__init__:108 - Found earlystop_kwargs in TrainerSettings. Set to None if you dont want earlystopping.\n"
     ]
    }
   ],
   "source": [
    "trainer = train_model.Trainer(\n",
    "    model=model, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=train_dataloader, \n",
    "    validdataloader=test_dataloader, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:21<00:00, 43.61it/s]\n",
      "2023-05-23 10:25:45.386 | INFO     | src.models.train_model:report:203 - Epoch 0 train 0.4899 test 0.4026 metric ['0.8560']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:21<00:00, 44.46it/s]\n",
      "2023-05-23 10:26:08.390 | INFO     | src.models.train_model:report:203 - Epoch 1 train 0.3534 test 0.3930 metric ['0.8584']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:24<00:00, 38.21it/s]\n",
      "2023-05-23 10:26:35.053 | INFO     | src.models.train_model:report:203 - Epoch 2 train 0.3154 test 0.3944 metric ['0.8504']\n",
      "2023-05-23 10:26:35.056 | INFO     | src.models.train_model:__call__:423 - best loss: 0.3929879514464907, current loss 0.394427. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:23<00:00, 40.38it/s]\n",
      "2023-05-23 10:27:00.223 | INFO     | src.models.train_model:report:203 - Epoch 3 train 0.2924 test 0.3446 metric ['0.8730']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:25<00:00, 36.30it/s]\n",
      "2023-05-23 10:27:28.530 | INFO     | src.models.train_model:report:203 - Epoch 4 train 0.2734 test 0.3610 metric ['0.8727']\n",
      "2023-05-23 10:27:28.533 | INFO     | src.models.train_model:__call__:423 - best loss: 0.3445501288600788, current loss 0.360955. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:37<00:00, 25.23it/s]\n",
      "2023-05-23 10:28:08.327 | INFO     | src.models.train_model:report:203 - Epoch 5 train 0.2589 test 0.3416 metric ['0.8771']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:24<00:00, 38.01it/s]\n",
      "2023-05-23 10:28:34.942 | INFO     | src.models.train_model:report:203 - Epoch 6 train 0.2460 test 0.3642 metric ['0.8709']\n",
      "2023-05-23 10:28:34.944 | INFO     | src.models.train_model:__call__:423 - best loss: 0.3416468537157508, current loss 0.364227. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:24<00:00, 38.12it/s]\n",
      "2023-05-23 10:29:01.628 | INFO     | src.models.train_model:report:203 - Epoch 7 train 0.2354 test 0.3261 metric ['0.8873']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:40<00:00, 23.04it/s]\n",
      "2023-05-23 10:29:44.948 | INFO     | src.models.train_model:report:203 - Epoch 8 train 0.2289 test 0.3625 metric ['0.8761']\n",
      "2023-05-23 10:29:44.951 | INFO     | src.models.train_model:__call__:423 - best loss: 0.326098142013808, current loss 0.362503. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:30<00:00, 31.17it/s]\n",
      "2023-05-23 10:30:17.434 | INFO     | src.models.train_model:report:203 - Epoch 9 train 0.2126 test 0.3338 metric ['0.8904']\n",
      "2023-05-23 10:30:17.437 | INFO     | src.models.train_model:__call__:423 - best loss: 0.326098142013808, current loss 0.333785. Counter 2.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [04:55<00:00, 29.53s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have the latest model at trainer.model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look at the settings.earlystop_kwargs, you can see that save is by default false. If you change this to true, the trainer would have kept track of the best model so far and saved it in between. Because this can take up additional time and in a learning setting like we are in we typically dont really want to save the model for later use, we dont need it here.\n",
    "\n",
    "However, in a real life setting you probably want the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.earlystop_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldir exists: True\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\").resolve()\n",
    "print(f'modeldir exists: {model_dir.exists()}')\n",
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(trainer.model, modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case you would have set the earlystop.save to true like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train_dataloader),\n",
    "    valid_steps=len(test_dataloader),\n",
    "    tunewrite=[\"tensorboard\"],\n",
    "    earlystop_kwargs={'save': True, 'verbose': True, 'patience': 10}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer would have saved checkpoints of the last best model. You can obtain the location of the checkpoint with `trainer.early_stopping.path`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.0625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "acc.item() * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 10:40:12.331 | INFO     | src.data.data_tools:clean_dir:104 - Clean out /workspaces/ML22/models/test\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
