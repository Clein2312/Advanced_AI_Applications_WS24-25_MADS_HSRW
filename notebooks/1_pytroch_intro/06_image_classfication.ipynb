{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11985b6d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 12:29:53.022 | INFO     | src.models.train_model:trainloop:45 - Epoch 0 train 0.0076 | test 0.0066\n",
      "2022-04-25 12:30:05.484 | INFO     | src.models.train_model:trainloop:45 - Epoch 1 train 0.0057 | test 0.0060\n",
      "2022-04-25 12:30:18.560 | INFO     | src.models.train_model:trainloop:45 - Epoch 2 train 0.0050 | test 0.0059\n",
      "2022-04-25 12:30:31.716 | INFO     | src.models.train_model:trainloop:45 - Epoch 3 train 0.0046 | test 0.0057\n",
      "2022-04-25 12:30:44.825 | INFO     | src.models.train_model:trainloop:45 - Epoch 4 train 0.0043 | test 0.0051\n",
      "2022-04-25 12:30:57.773 | INFO     | src.models.train_model:trainloop:45 - Epoch 5 train 0.0041 | test 0.0052\n",
      "2022-04-25 12:31:10.671 | INFO     | src.models.train_model:trainloop:45 - Epoch 6 train 0.0039 | test 0.0052\n",
      "2022-04-25 12:31:23.599 | INFO     | src.models.train_model:trainloop:45 - Epoch 7 train 0.0037 | test 0.0051\n",
      "2022-04-25 12:31:36.553 | INFO     | src.models.train_model:trainloop:45 - Epoch 8 train 0.0035 | test 0.0051\n",
      "2022-04-25 12:31:49.560 | INFO     | src.models.train_model:trainloop:45 - Epoch 9 train 0.0034 | test 0.0053\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from src.models import train_model\n",
    "model = train_model.trainloop(\n",
    "    epochs=10,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "accuracy.item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = False\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    import shutil\n",
    "    # remove all the data\n",
    "    data = (Path(datadir) / \"FashionMNIST\")\n",
    "    if data.exists():\n",
    "        shutil.rmtree(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f991136b32d0e931878e2d826f60fb70ad3d0a23fd6e1a56ea114087d779837"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deep-learning-uo9RXddf-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
