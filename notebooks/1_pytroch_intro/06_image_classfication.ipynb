{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1630c2a60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"../../models/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 15:20:25.169 | INFO     | src.data.data_tools:clean_dir:164 - Clean out ../../models/test\n",
      "2022-05-05 15:20:35.937 | INFO     | src.models.train_model:trainloop:75 - Epoch 0 train 0.0077 | test 0.0068 acc 0.8512\n",
      "2022-05-05 15:20:46.748 | INFO     | src.models.train_model:trainloop:75 - Epoch 1 train 0.0056 | test 0.0053 acc 0.8763\n",
      "2022-05-05 15:20:58.441 | INFO     | src.models.train_model:trainloop:75 - Epoch 2 train 0.0050 | test 0.0057 acc 0.8653\n",
      "2022-05-05 15:21:09.450 | INFO     | src.models.train_model:trainloop:75 - Epoch 3 train 0.0047 | test 0.0053 acc 0.8784\n",
      "2022-05-05 15:21:20.727 | INFO     | src.models.train_model:trainloop:75 - Epoch 4 train 0.0044 | test 0.0054 acc 0.8712\n"
     ]
    }
   ],
   "source": [
    "model = train_model.trainloop(\n",
    "    epochs=5,\n",
    "    model=model,\n",
    "    optimizer=optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    log_dir=log_dir,\n",
    "    loss_fn=loss_fn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    eval_steps=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.1875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "accuracy.item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 18:44:37.145 | INFO     | src.data.data_tools:clean_dir:121 - Clean out {dir}\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f991136b32d0e931878e2d826f60fb70ad3d0a23fd6e1a56ea114087d779837"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deep-learning-uo9RXddf-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
