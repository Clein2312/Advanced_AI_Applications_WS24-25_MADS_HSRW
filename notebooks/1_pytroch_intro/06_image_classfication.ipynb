{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "datadir = \"../../data/raw/\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=datadir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, int)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = training_data[0]\n",
    "type(x), type(x[0]), type(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13876b0d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 256]         131,328\n",
      "              ReLU-5                  [-1, 256]               0\n",
      "            Linear-6                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.04\n",
      "Estimated Total Size (MB): 2.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 60000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"../../models/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 21:01:54.845 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/test/20230425-2101\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:13<00:00, 68.79it/s]\n",
      "2023-04-25 21:02:08.850 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.4950 test 0.4195 metric ['0.8503']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:14<00:00, 64.97it/s]\n",
      "2023-04-25 21:02:23.708 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.3545 test 0.4423 metric ['0.8366']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:16<00:00, 56.81it/s]\n",
      "2023-04-25 21:02:40.704 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3135 test 0.3699 metric ['0.8750']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:17<00:00, 54.15it/s]\n",
      "2023-04-25 21:02:58.502 | INFO     | src.models.train_model:trainloop:180 - Epoch 3 train 0.2973 test 0.3227 metric ['0.8900']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 938/938 [00:18<00:00, 50.24it/s]\n",
      "2023-04-25 21:03:17.682 | INFO     | src.models.train_model:trainloop:180 - Epoch 4 train 0.2816 test 0.3478 metric ['0.8756']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [01:22<00:00, 16.57s/it]\n"
     ]
    }
   ],
   "source": [
    "model, test_loss = train_model.trainloop(\n",
    "    epochs=5,\n",
    "    model=model,\n",
    "    optimizer=optim.Adam,\n",
    "    learning_rate=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy],\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    log_dir=log_dir,\n",
    "    train_steps=len(train_dataloader),\n",
    "    eval_steps=50,\n",
    "    tunewriter=[\"tensorboard\", \"gin\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\") \n",
    "model_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "acc.item() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 16:04:03.626 | INFO     | src.data.data_tools:clean_dir:195 - Clean out ../../models/test\n"
     ]
    }
   ],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
