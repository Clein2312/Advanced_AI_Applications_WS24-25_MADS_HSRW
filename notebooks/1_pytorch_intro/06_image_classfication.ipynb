{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow for a data science project will follow these lines:\n",
    "\n",
    "1. Get and explore the data\n",
    "2. Build a model \n",
    "3. Train the model\n",
    "4. Save and predict\n",
    "\n",
    "## 1. Get and Explore the Data\n",
    "The first step can take quite some time; data quality is often something that needs to be checked, and correlations between data should often be explored and visualized.\n",
    "\n",
    "This step can be a full project on its own: you clean the data, make sure you can access it properly, and create visualizations and hypothesis to gain insight into the data that can be shown in a dashboard.\n",
    "\n",
    "The insight in the data is an essential ingredient for deciding on a model.\n",
    "\n",
    "## 2. Build a model\n",
    "Based on domain knowledge and a first exploration of the data, a model can be selected.\n",
    "\n",
    "Sometimes, the relation between features and outcome is very obvious. You might have features that\n",
    "correlate very high with the outcome variable, and a domain expert confirms that the correlations make sense.\n",
    "\n",
    "If this is the case, you can often build a simple model. If you expect to have non-linear and complex interactions between the features,\n",
    "you could use a model that works with non-linear data like a SVM plus kernel, or a random forest.\n",
    "\n",
    "If you have enough data (as a rule of thumb, a lower threshold of 1000 observations) you can consider a neural network architecture.\n",
    "If the expected complexity of the data is low, you can use a relative small network.\n",
    "If you have lots and lots of data with a high complexity, you should consider to increase the complexity of your model too.\n",
    "\n",
    "How you can build a model, and what suitable models are for different datatypes and situations, will be the subject of the whole course.\n",
    "\n",
    "## 3. Train the model\n",
    "Once you created a model, it hasnt learned anything yet. The model must be trained to learn the right connections, a bit like a baby that has to learn about what works and what doesn't.\n",
    "\n",
    "In this notebook, I will introduce you to PyTorch. Another high level library is Tensorflow, which is used a lot too.\n",
    "While the interface is comparable, the Tensorflow syntax is a bit more high-level. While this can be an advantage, \n",
    "it also has a downside: at the moment you ever need to dive a bit deeper into the architecture itself, it is much harder to\n",
    "add something new with TensorFlow, compared to PyTorch.\n",
    "\n",
    "## 4. Save and predict\n",
    "Finally, you will want to use the trained model to predict new observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "We will use the fashion MNIST dataset. You will find this dataset a lot in machine learning tutorials. It are small (28x28) images of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/rgrouls/code/ML22 to sys.path, this is necessary to import from src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/rgrouls/code/ML22',\n",
       " '/Users/rgrouls/code/ML22/notebooks/1_pytroch_intro',\n",
       " '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python39.zip',\n",
       " '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9',\n",
       " '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/rgrouls/Library/Caches/pypoetry/virtualenvs/deep-learning-HUU8cknU-py3.9/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "srcdir = Path(\"../..\").resolve()\n",
    "print(f\"Adding {srcdir} to sys.path, this is necessary to import from src\")\n",
    "sys.path.insert(0, str(srcdir))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 11:10:01.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset already exists at /Users/rgrouls/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2023-06-06 11:10:01.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mDigest of downloaded /Users/rgrouls/.cache/mads_datasets/fashionmnist/fashionmnist.pt matches expected digest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "fashionfactory = DatasetFactoryProvider.get_factory(DatasetType.FASHION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = fashionfactory.create_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `torch.datasets`. They implement at minimum an `.__getitem__` and `.__len__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mads_datasets.datasets.MNISTDataset at 0x13f38acd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor(9, dtype=torch.uint8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, we can use the __getitem__ method by calling an index, just like you would do with a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, torch.Tensor, torch.Tensor, tensor(1.), tensor(0.), torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = datasets[\"train\"][0]\n",
    "type(x), type(x[0]), type(x[1]), x[0].max(), x[0].min(), x[0].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets[\"train\"].__getitem__(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a tuple. We can check the length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the 0th item, which is the image (tensor). The other item is the label (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = x[0]\n",
    "img.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image has a channel-first convention: it is a 28x28 pixel image, and it has 1 channel (grey). Look into the official documentation if you want to know more about datasets and how to build your own: [docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Ok, we want to batch this into a dataloader. From the documentation:\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the length of the dataloader different from the dataset? We had 60000 items before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can either use pytorches DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(datasets[\"train\"], batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(datasets[\"valid\"], batch_size=64, shuffle=True)\n",
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45 ms ± 255 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X, y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 11:10:14.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset already exists at /Users/rgrouls/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2023-06-06 11:10:14.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mDigest of downloaded /Users/rgrouls/.cache/mads_datasets/fashionmnist/fashionmnist.pt matches expected digest\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58 ms ± 14.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# or the BaseDatastreamer from the datasetfactory. Check out which one is faster\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "%timeit X, y = next(iter(trainstreamer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 156)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(trainstreamer))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do we see here? Our datashape has four dimensions:\n",
    "\n",
    "- 64: this is the batch size. Every batch has 64 observations; in this case 64 images\n",
    "- 1: this is the channel. Colorimages typically have 3 channels. Our images have just one color, and thus 1 channel. So images can have more channels (e.g. infrared etc)\n",
    "- (28,28) : this is the actual image, with dimensions 28x28\n",
    "\n",
    "Lets visualize the first example, the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = X[1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1401f4cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 256]         131,328\n",
      "              ReLU-5                  [-1, 256]               0\n",
      "            Linear-6                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.04\n",
      "Estimated Total Size (MB): 2.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you recognize the setup from the `linearmodel` notebook. \n",
    "\n",
    "- We will `Flatten` the image. That means we will transform our (64, 1, 28, 28) data into (64, 784) shaped data. What we do here, is flattening the image into a one dimensional vector.\n",
    "- We have a stack of hidden layers. These are essentially dotproducts. Our vector of 784 (28*28) elements is transformed into 512 elements, and then into 10 elements because we have 10 classes.\n",
    "- in between the linear transformations you can see the activation functions,here a `ReLu` \n",
    "- The `forward` method is what is called during training. This gives you control over the flow of information: it is easy to create some parallel flow of data if you want to do something like that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer. We will dive into this in later lessons.\n",
    "\n",
    "For now, it is enough to know this:\n",
    "\n",
    "Your model makes a prediction. But how does the model know if it is right, or wrong?\n",
    "And, more specific: how does the model know which weights it needs to modify in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "log_dir=Path(\"../../models/test\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /Users/rgrouls/code/ML22/models/test\n",
       "train_steps: 937\n",
       "valid_steps: 156\n",
       "reporttypes: [<ReportTypes.TENSORBOARD: 2>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.1, 'patience': 10}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 11:10:57.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.data_tools\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mLogging to /Users/rgrouls/code/ML22/models/test/20230606-1110\u001b[0m\n",
      "\u001b[32m2023-06-06 11:10:57.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.train_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = train_model.Trainer(\n",
    "    model=model, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=trainstreamer, \n",
    "    validdataloader=validstreamer, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have the latest model at trainer.model, or just use the old model (which is the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look at the settings.earlystop_kwargs, you can see that save is by default false. If you change this to true, the trainer would have kept track of the best model so far and saved it in between. Because this can take up additional time and in a learning setting like we are in we typically dont really want to save the model for later use, we dont need it here.\n",
    "\n",
    "However, in a real life setting you probably want the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.earlystop_kwargs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldir exists: True\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"../../models\").resolve()\n",
    "print(f'modeldir exists: {model_dir.exists()}')\n",
    "modelpath = model_dir / \"trained_model\"\n",
    "torch.save(model, modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case you would have set the earlystop.save to true like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train_dataloader),\n",
    "    valid_steps=len(test_dataloader),\n",
    "    tunewrite=[\"tensorboard\"],\n",
    "    earlystop_kwargs={'save': True, 'verbose': True, 'patience': 10}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer would have saved checkpoints of the last best model. You can obtain the location of the checkpoint with `trainer.early_stopping.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/ML22/models/test/20230602-1037/checkpoint.pt')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.early_stopping.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(modelpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch $X$, $y$ and make a prediction $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(testloader))\n",
    "yhat = loaded_model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy:\n",
    "- for every example we have 10 numbers\n",
    "- the location with the highest value is the prediction\n",
    "- we can get the index with `argmax` over dimension 1\n",
    "- we compare that index with the original number\n",
    "- This gives us a count of all the correct predictions\n",
    "- dividing that through the total length gives us the accuracy percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.75"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (yhat.argmax(dim=1) == y).sum() / len(y)\n",
    "acc.item() * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the accuracy for a single batch! \n",
    "Get another batch by running next() in the cell above, and calculate the accuracy again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = True\n",
    "from src.data import data_tools\n",
    "# to remove the trained model\n",
    "if cleanup:\n",
    "    modelpath.unlink()\n",
    "    data_tools.clean_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
