{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools, make_dataset\n",
    "from src.models import tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset. This is the MNIST for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:40:12.527 | INFO     | src.data.make_dataset:get_imdb_data:95 - ../../data/raw/aclImdb already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../data/raw\"\n",
    "trainpaths, testpaths = make_dataset.get_imdb_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 50k movie reviews, labeled positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testpaths), len(trainpaths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look at the first datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 15409.56it/s]\n",
      "100%|██████████| 25000/25000 [00:01<00:00, 16926.87it/s]\n"
     ]
    }
   ],
   "source": [
    "traindataset = data_tools.TextDataset(paths=trainpaths)\n",
    "testdataset = data_tools.TextDataset(paths=testpaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sorry, but every time I see a film wherein a woman sucker-punches a man and the man does nothing but cower, the film looses all credibility. So the new (female) Starbuck immediately tainted the plot before it even got off the ground (no pun intended). Dirk Benedict was so much more plausible as the sensitive hero-type than the new-age Kattee Sackhoff-- whose overacting will probably be henceforth lauded as \"a compelling, exciting, must-see, ground-breaking performance,\" by the politically correct new-speak of today\\'s review copy editors; but in essence, it is just a tired, old image of a woman with a chip on her shoulder as big as a townhouse: the biggest cliché on screens today. I may give this series one more shot, but human caricatures alone will not keep me tuned in. As James Hilton once bemoaned, \"A story, please; just give me a story.\"',\n",
       " 'neg')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[0]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy data. We have Uppercase, punctuation, and even html tags. Let's clean that out in order to reduce dimensionality, without loosing too much information about the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = f\"[{string.punctuation}]\"\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    punctuation = f\"[{string.punctuation}]\"\n",
    "    # remove CaPiTaLs\n",
    "    lowercase = text.lower()\n",
    "    # change don't and isn't into dont and isnt\n",
    "    neg = re.sub(\"\\\\'\", \"\", lowercase)\n",
    "    # swap html tags for spaces\n",
    "    html = re.sub(\"<br />\", \" \", neg)\n",
    "    # swap punctuation for spaces\n",
    "    stripped = re.sub(punctuation, \" \", html)\n",
    "    # remove extra spaces\n",
    "    spaces = re.sub(\"  +\", \" \", stripped)\n",
    "    return spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sorry but every time i see a film wherein a woman sucker punches a man and the man does nothing but cower the film looses all credibility so the new female starbuck immediately tainted the plot before it even got off the ground no pun intended dirk benedict was so much more plausible as the sensitive hero type than the new age kattee sackhoff whose overacting will probably be henceforth lauded as a compelling exciting must see ground breaking performance by the politically correct new speak of todays review copy editors but in essence it is just a tired old image of a woman with a chip on her shoulder as big as a townhouse the biggest cliché on screens today i may give this series one more shot but human caricatures alone will not keep me tuned in as james hilton once bemoaned a story please just give me a story ',\n",
       " 'neg')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(x), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now we need to create a vocabulary, which is a mapping from every unique word to an arbitrary integer. We have seen this in lesson 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:40:28.891 | INFO     | src.models.tokenizer:build_vocab:23 - Found 79808 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "v = tokenizer.build_vocab(corpus, max=10000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after cleaning, we have about 80k unique tokens. This is even more without the cleaning, because \"The\" and \"the\" will be two different tokens.\n",
    "\n",
    "We also have tokens for unknown words, and for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[\"<UNK>\"], v[\"<PAD>\"], v[\"sdflkjl\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps a sentence of words to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 241, 33, 7415, 500, 966, 187, 19, 4321, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v[word] for word in clean(x).split()[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Callable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self, max: int, vocab: Vocab, clean: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.max = max\n",
    "        self.vocab = vocab\n",
    "        self.clean = clean\n",
    "\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"neg\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def __call__(self, batch: List) -> Tuple[Tensor, Tensor]:\n",
    "        labels, text = [], []\n",
    "        for x, y in batch:\n",
    "            if clean is not None:\n",
    "                x = self.clean(x)\n",
    "            x = x.split()[: self.max]\n",
    "            tokens = torch.tensor([self.vocab[word] for word in x])\n",
    "            text.append(tokens)\n",
    "            labels.append(self.cast_label(y))\n",
    "\n",
    "        text_ = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "        return text_, torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is necessary to:\n",
    "- cut of long sentences to get equal length. 100 words will be enough to get the sentiment in most cases\n",
    "- we need to cast the labels \"neg\" and \"pos\" to integers\n",
    "- we also pad if a sentence is shorter than the max lenght\n",
    "\n",
    "We can feed the preprocessor to the default dataloader from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "preprocessor = Preprocessor(max=100, vocab=v, clean=clean)\n",
    "dataloader = DataLoader(\n",
    "    traindataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get batched sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 682,  601,    5,    2,  963, 1901,   88,    5,  934, 2827,    2,   19,\n",
       "          15,   29, 5084,   25,  161,  328,    4, 5082,    5, 2352,    1,    2,\n",
       "          88, 6452,    1,    7,    8,    2,  649,  197,    1,    3, 9855, 3437,\n",
       "         139,    8,    2, 2587,    2, 1566,   13,    1,  618,    8,    4,   74,\n",
       "         636,  133,  226,   37,   33, 5505,  105,   26,   13,   33, 2807, 5906,\n",
       "           3,    4,  532,    1, 1244, 1257,   35, 1661, 2313,    6,   27, 3337,\n",
       "          81,    4, 6034,  321,   26,   13,   21,    2,  375,    1, 2402,  129,\n",
       "          26,   67,   33, 1089,  600,    8, 1063,    3,   38,  107,   82,   80,\n",
       "           8,    2, 5259, 1113])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
