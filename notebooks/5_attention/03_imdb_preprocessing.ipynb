{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/rgrouls/code/ML22 to sys.path, this is necessary to import from src\n",
      "['/Users/rgrouls/code/ML22', '/Users/rgrouls/code/ML22', '/Users/rgrouls/Library/Caches/pypoetry/virtualenvs/deep-learning-HUU8cknU-py3.9/lib/python3.9/site-packages/ray/thirdparty_files', '/Users/rgrouls/code/ML22', '/Users/rgrouls/code/ML22/notebooks/5_attention', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python39.zip', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9/lib-dynload', '', '/Users/rgrouls/Library/Caches/pypoetry/virtualenvs/deep-learning-HUU8cknU-py3.9/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "srcdir = Path(\"../..\").resolve()\n",
    "print(f\"Adding {srcdir} to sys.path, this is necessary to import from src\")\n",
    "sys.path.insert(0, str(srcdir))\n",
    "print(sys.path)\n",
    "\n",
    "from src.models import tokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset. This is the MNIST for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "imdbdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.IMDB)\n",
    "datasets = imdbdatasetfactory.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = datasets[\"train\"]\n",
    "testdataset = datasets[\"valid\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 50k movie reviews, labeled positive or negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look at the first datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This was the second entry in the regular Columbo series, and it holds up well today. As I am able to look at it closely now on DVD and see how it is constructed, I am very impressed with the direction of Bernard L. Kowalski (who directed the fine MACHO CALLAHAN as well as countless TV episodes)--watch how the post-murder actions of the killer are shown on a split-screen effect on his two eyeglasses, watch how the murder itself is shown in montage fashion, watch the point-of-view shot from the perspective of the corpse. Also, the wild but impressive avant-garde musical score from noted jazzman Gil Melle was incredible and helped so much to create atmosphere. And the supporting performance of Brett Halsey as the golf pro was wonderful--such subtlety and complexity in a role that nine out of ten times would be a one-dimensional cutout. The \"formula\" had not yet been set when this episode was filmed, so there are still some surprises in Columbo\\'s methods. Of course, Falk, Robert Culp, and Ray Milland are the highest-quality actors and it\\'s a pleasure to see them work--all men are familiar from many other roles yet lose themselves in their characters here. In all, this entry in the Columbo series--and MANY of the others--are as well-crafted as a very good feature film.',\n",
       " 'pos')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[0]\n",
    "x, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy data. We have Uppercase, punctuation, and even html tags. Let's clean that out in order to reduce dimensionality, without loosing too much information about the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = f\"[{string.punctuation}]\"\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    punctuation = f\"[{string.punctuation}]\"\n",
    "    # remove CaPiTaLs\n",
    "    lowercase = text.lower()\n",
    "    # change don't and isn't into dont and isnt\n",
    "    neg = re.sub(\"\\\\'\", \"\", lowercase)\n",
    "    # swap html tags for spaces\n",
    "    html = re.sub(\"<br />\", \" \", neg)\n",
    "    # swap punctuation for spaces\n",
    "    stripped = re.sub(punctuation, \" \", html)\n",
    "    # remove extra spaces\n",
    "    spaces = re.sub(\"  +\", \" \", stripped)\n",
    "    return spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('this was the second entry in the regular columbo series and it holds up well today as i am able to look at it closely now on dvd and see how it is constructed i am very impressed with the direction of bernard l kowalski who directed the fine macho callahan as well as countless tv episodes watch how the post murder actions of the killer are shown on a split screen effect on his two eyeglasses watch how the murder itself is shown in montage fashion watch the point of view shot from the perspective of the corpse also the wild but impressive avant garde musical score from noted jazzman gil melle was incredible and helped so much to create atmosphere and the supporting performance of brett halsey as the golf pro was wonderful such subtlety and complexity in a role that nine out of ten times would be a one dimensional cutout the formula had not yet been set when this episode was filmed so there are still some surprises in columbos methods of course falk robert culp and ray milland are the highest quality actors and its a pleasure to see them work all men are familiar from many other roles yet lose themselves in their characters here in all this entry in the columbo series and many of the others are as well crafted as a very good feature film ',\n",
       " 'pos')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(x), y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now we need to create a vocabulary, which is a mapping from every unique word to an arbitrary integer. We have seen this in lesson 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 12:05:59.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.tokenizer\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mFound 79808 tokens\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "v = tokenizer.build_vocab(corpus, max=10000)\n",
    "len(v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after cleaning, we have about 80k unique tokens. This is even more without the cleaning, because \"The\" and \"the\" will be two different tokens.\n",
    "\n",
    "We also have tokens for unknown words, and for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[\"<UNK>\"], v[\"<PAD>\"], v[\"sdflkjl\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps a sentence of words to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4929, 17, 48, 66, 130, 80, 20, 4, 1494, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v[word] for word in clean(x).split()[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Callable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self, max: int, vocab: Vocab, clean: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.max = max\n",
    "        self.vocab = vocab\n",
    "        self.clean = clean\n",
    "\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"neg\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def __call__(self, batch: List) -> Tuple[Tensor, Tensor]:\n",
    "        labels, text = [], []\n",
    "        for x, y in batch:\n",
    "            if clean is not None:\n",
    "                x = self.clean(x)\n",
    "            x = x.split()[: self.max]\n",
    "            tokens = torch.tensor([self.vocab[word] for word in x])\n",
    "            text.append(tokens)\n",
    "            labels.append(self.cast_label(y))\n",
    "\n",
    "        text_ = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "        return text_, torch.tensor(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is necessary to:\n",
    "- cut of long sentences to get equal length. 100 words will be enough to get the sentiment in most cases\n",
    "- we need to cast the labels \"neg\" and \"pos\" to integers\n",
    "- we also pad if a sentence is shorter than the max lenght\n",
    "\n",
    "We can feed the preprocessor to the default dataloader from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "preprocessor = Preprocessor(max=100, vocab=v, clean=clean)\n",
    "dataloader = DataLoader(\n",
    "    traindataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get batched sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  21,    4,  251,   19,   18,   19,   15,    2, 1360,   28,    5,    2,\n",
       "         246,   96,  122,   90,   32,    4,  909,  163,   42,  251,  163, 1790,\n",
       "           1,  327,  149,  120,  251, 8451,    5,   74, 1292,  295,    1,  482,\n",
       "         398, 2633,   61,   55,  280,    1,   61,    1,  101, 5158, 5313,  381,\n",
       "           6,  109,   20,  262, 3520,    3,  404,   21,  159,   10,  593,   86,\n",
       "         285,   49, 1493, 3228,    6,  190,  169,    8,    9,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this code is wrapped into the DatasetFactoryProvider, which you can see in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
