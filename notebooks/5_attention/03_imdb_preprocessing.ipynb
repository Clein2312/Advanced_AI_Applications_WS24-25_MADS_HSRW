{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools, make_dataset\n",
    "from src.models import tokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset. This is the MNIST for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import DatasetFactoryProvider, DatasetType\n",
    "imdbdatasetfactory = DatasetFactoryProvider.get_factory(DatasetType.IMDB)\n",
    "datasets = imdbdatasetfactory.create_dataset()\n",
    "traindataset = datasets[\"train\"]\n",
    "testdataset = datasets[\"test\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of 50k movie reviews, labeled positive or negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look at the first datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"My observations: vamp outfit at end is ravishing and wonderful, exotic and fantastic. Jeanette wore it well, and got even with naive Nelson. Boat crashing into his balcony served him right. Costume outfits of his female mafia were designed surprisingly well, especially by today's standards. 1942 costume designer did great job. Main song theme just lovely.<br /><br />Caution to negative posters: 1942 was time of WW II; Pearl Harbor happened year before. U.S. just coming out of Great Depression; needed to get out and spend that hard earned money on diversion of singing, dance and yes, fantastic fantasy. Despotic dictators were trying to rule out there in RL, snuffing out freedoms. Thank goodness the public had these fantastic plot line movies to attend. Movie going was a privileged treat, in those depressing times. When you, negative posters, become actors or even movie stars, then YOU have room to talk and criticize. Jeanette's and Nelson's movies stand the test of time.<br /><br />Angel wings wonderful, on the real angel. RL wings at costume party not so hot, but great on Jeanette considering the SL.<br /><br />Beautiful singing by Jeanette and Nelson, as always. Jeanette dancing was a pure delight.<br /><br />15/10\",\n",
       " 'pos')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[0]\n",
    "x, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy data. We have Uppercase, punctuation, and even html tags. Let's clean that out in order to reduce dimensionality, without loosing too much information about the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = f\"[{string.punctuation}]\"\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    punctuation = f\"[{string.punctuation}]\"\n",
    "    # remove CaPiTaLs\n",
    "    lowercase = text.lower()\n",
    "    # change don't and isn't into dont and isnt\n",
    "    neg = re.sub(\"\\\\'\", \"\", lowercase)\n",
    "    # swap html tags for spaces\n",
    "    html = re.sub(\"<br />\", \" \", neg)\n",
    "    # swap punctuation for spaces\n",
    "    stripped = re.sub(punctuation, \" \", html)\n",
    "    # remove extra spaces\n",
    "    spaces = re.sub(\"  +\", \" \", stripped)\n",
    "    return spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my observations vamp outfit at end is ravishing and wonderful exotic and fantastic jeanette wore it well and got even with naive nelson boat crashing into his balcony served him right costume outfits of his female mafia were designed surprisingly well especially by todays standards 1942 costume designer did great job main song theme just lovely caution to negative posters 1942 was time of ww ii pearl harbor happened year before u s just coming out of great depression needed to get out and spend that hard earned money on diversion of singing dance and yes fantastic fantasy despotic dictators were trying to rule out there in rl snuffing out freedoms thank goodness the public had these fantastic plot line movies to attend movie going was a privileged treat in those depressing times when you negative posters become actors or even movie stars then you have room to talk and criticize jeanettes and nelsons movies stand the test of time angel wings wonderful on the real angel rl wings at costume party not so hot but great on jeanette considering the sl beautiful singing by jeanette and nelson as always jeanette dancing was a pure delight 15 10',\n",
       " 'pos')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(x), y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now we need to create a vocabulary, which is a mapping from every unique word to an arbitrary integer. We have seen this in lesson 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:53:20.565 | INFO     | src.models.tokenizer:build_vocab:27 - Found 79808 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "v = tokenizer.build_vocab(corpus, max=10000)\n",
    "len(v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after cleaning, we have about 80k unique tokens. This is even more without the cleaning, because \"The\" and \"the\" will be two different tokens.\n",
    "\n",
    "We also have tokens for unknown words, and for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[\"<UNK>\"], v[\"<PAD>\"], v[\"sdflkjl\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps a sentence of words to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 513, 1751, 5, 11, 119, 3, 2, 1137, 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v[word] for word in clean(x).split()[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Callable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(\n",
    "        self, max: int, vocab: Vocab, clean: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.max = max\n",
    "        self.vocab = vocab\n",
    "        self.clean = clean\n",
    "\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        if label == \"neg\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def __call__(self, batch: List) -> Tuple[Tensor, Tensor]:\n",
    "        labels, text = [], []\n",
    "        for x, y in batch:\n",
    "            if clean is not None:\n",
    "                x = self.clean(x)\n",
    "            x = x.split()[: self.max]\n",
    "            tokens = torch.tensor([self.vocab[word] for word in x])\n",
    "            text.append(tokens)\n",
    "            labels.append(self.cast_label(y))\n",
    "\n",
    "        text_ = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "        return text_, torch.tensor(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is necessary to:\n",
    "- cut of long sentences to get equal length. 100 words will be enough to get the sentiment in most cases\n",
    "- we need to cast the labels \"neg\" and \"pos\" to integers\n",
    "- we also pad if a sentence is shorter than the max lenght\n",
    "\n",
    "We can feed the preprocessor to the default dataloader from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "preprocessor = Preprocessor(max=100, vocab=v, clean=clean)\n",
    "dataloader = DataLoader(\n",
    "    traindataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get batched sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "x.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  11,   17,    7,   21,  125,  796,  187,   17,    9,   45,   46, 2726,\n",
       "         482,    3,  334,  135,   60,   68,   27,  428,    5, 2831,   10, 1272,\n",
       "         419,    2,   17,   85,    5,   29,  121, 1553,  482,   29,    1,   21,\n",
       "           4, 1158,  622,   17,   60,    7,   49,   44,   22,   89,  181,    6,\n",
       "          27, 1754,    3, 7197, 3526,   10,  419,    2,  236,    5,    2,    1,\n",
       "         227,    3,    5, 6428,   52,  867,   10,  102,  142,  166,    6,    1,\n",
       "          58, 1152,  757,   38,   12,  243,    8, 4511,   52,  640,  555,  442,\n",
       "         286, 2754,   15,    2, 2726,  482,    3,  698,  796,  187,  586,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this code is wrapped into the DatasetFactoryProvider, which you can see in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
