{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/rgrouls/code/ML22 to sys.path, this is necessary to import from src\n",
      "['/Users/rgrouls/code/ML22', '/Users/rgrouls/code/ML22/notebooks/5_attention', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python39.zip', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9', '/Users/rgrouls/.pyenv/versions/3.9.16/lib/python3.9/lib-dynload', '', '/Users/rgrouls/Library/Caches/pypoetry/virtualenvs/deep-learning-HUU8cknU-py3.9/lib/python3.9/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='imdb.gin', imports=[], includes=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "srcdir = Path(\"../..\").resolve()\n",
    "print(f\"Adding {srcdir} to sys.path, this is necessary to import from src\")\n",
    "sys.path.insert(0, str(srcdir))\n",
    "print(sys.path)\n",
    "\n",
    "from src.models import tokenizer, train_model\n",
    "from src.models import metrics\n",
    "from src.models.rnn_models import NLPmodel, AttentionNLP\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "import gin\n",
    "gin.parse_config_file(\"imdb.gin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the streamers from the datasetfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 12:07:11.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset already exists at /Users/rgrouls/.cache/mads_datasets/imdb\u001b[0m\n",
      "\u001b[32m2023-06-06 12:07:11.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mDigest of downloaded /Users/rgrouls/.cache/mads_datasets/imdb/aclImdb_v1.tar.gz matches expected digest\u001b[0m\n",
      "\u001b[32m2023-06-06 12:07:15.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mcreate_dataset\u001b[0m:\u001b[36m255\u001b[0m - \u001b[1mCreating TextDatasets from 25000 trainfiles and 25000 testfiles.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:06<00:00, 3681.96it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 25000/25000 [00:06<00:00, 3687.94it/s]\n",
      "\u001b[32m2023-06-06 12:07:32.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.datasetfactory\u001b[0m:\u001b[36mbuild_vocab\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mFound 79808 tokens\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "imdbdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.IMDB)\n",
    "streamers = imdbdatasetfactory.create_datastreamer(batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches 781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([32]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = streamers[\"train\"]\n",
    "print(f\"number of batches {len(train)}\")\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = streamers[\"valid\"].stream()\n",
    "X, y = next(iter(trainstreamer))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset has 782 batches of 32 examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup accuracy and loss_fn (this is a classification problem with two classes, 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "log_dir = Path(\"../../models/attention/\").resolve()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic config. We need to specify the vocabulary lenght for the embedding layer.\n",
    "Trainsteps are set to just 100 batches for speedup in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /Users/rgrouls/code/ML22/models/attention\n",
       "train_steps: 100\n",
       "valid_steps: 25\n",
       "reporttypes: [<ReportTypes.TENSORBOARD: 2>, <ReportTypes.GIN: 1>]\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings, ReportTypes\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=100,\n",
    "    valid_steps=25,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gin.get_bindings(\"NLPmodel\")[\"config\"][\"vocab\"] == imdbdatasetfactory.settings.maxvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLPmodel(\n",
       "  (emb): Embedding(10000, 128)\n",
       "  (rnn): GRU(128, 128, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NLPmodel()\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base NLP model is just a GRU, with an embedding as a first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 14:01:09.977 | INFO     | src.data.data_tools:dir_add_timestamp:145 - Logging to /workspaces/ML22/models/attention/20230531-1401\n",
      "2023-05-31 14:01:09.989 | INFO     | src.models.train_model:__init__:108 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:31<00:00,  3.20it/s]\n",
      "2023-05-31 14:01:43.069 | INFO     | src.models.train_model:report:208 - Epoch 0 train 0.6918 test 0.6839 metric ['0.5537']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:24<00:00,  4.17it/s]\n",
      "2023-05-31 14:02:08.570 | INFO     | src.models.train_model:report:208 - Epoch 1 train 0.6838 test 0.6827 metric ['0.5763']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.81it/s]\n",
      "2023-05-31 14:02:37.146 | INFO     | src.models.train_model:report:208 - Epoch 2 train 0.6785 test 0.6666 metric ['0.6075']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:28<00:00,  3.54it/s]\n",
      "2023-05-31 14:03:07.514 | INFO     | src.models.train_model:report:208 - Epoch 3 train 0.6490 test 0.6681 metric ['0.5850']\n",
      "2023-05-31 14:03:07.518 | INFO     | src.models.train_model:__call__:245 - best loss: 0.6666410350799561, current loss 0.668060. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:29<00:00,  3.44it/s]\n",
      "2023-05-31 14:03:38.820 | INFO     | src.models.train_model:report:208 - Epoch 4 train 0.6806 test 0.6625 metric ['0.5975']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:22<00:00,  4.43it/s]\n",
      "2023-05-31 14:04:02.971 | INFO     | src.models.train_model:report:208 - Epoch 5 train 0.6809 test 0.6660 metric ['0.5725']\n",
      "2023-05-31 14:04:02.974 | INFO     | src.models.train_model:__call__:245 - best loss: 0.6625255990028381, current loss 0.665962. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:27<00:00,  3.68it/s]\n",
      "2023-05-31 14:04:34.726 | INFO     | src.models.train_model:report:208 - Epoch 6 train 0.6682 test 0.6616 metric ['0.6112']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.77it/s]\n",
      "2023-05-31 14:05:03.145 | INFO     | src.models.train_model:report:208 - Epoch 7 train 0.6548 test 0.7090 metric ['0.5800']\n",
      "2023-05-31 14:05:03.150 | INFO     | src.models.train_model:__call__:245 - best loss: 0.6615855002403259, current loss 0.709012. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.77it/s]\n",
      "2023-05-31 14:05:31.704 | INFO     | src.models.train_model:report:208 - Epoch 8 train 0.6530 test 0.6818 metric ['0.6275']\n",
      "2023-05-31 14:05:31.710 | INFO     | src.models.train_model:__call__:245 - best loss: 0.6615855002403259, current loss 0.681819. Counter 2.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:23<00:00,  4.23it/s]\n",
      "2023-05-31 14:05:57.022 | INFO     | src.models.train_model:report:208 - Epoch 9 train 0.5946 test 0.5922 metric ['0.6775']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [04:47<00:00, 28.70s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer = train_model.Trainer(\n",
    "    model=model, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=trainstreamer, \n",
    "    validdataloader=teststreamer, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the impact of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 14:07:01.145 | INFO     | src.data.data_tools:dir_add_timestamp:145 - Logging to /workspaces/ML22/models/attention/20230531-1407\n",
      "2023-05-31 14:07:01.159 | INFO     | src.models.train_model:__init__:108 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:25<00:00,  3.89it/s]\n",
      "2023-05-31 14:07:29.155 | INFO     | src.models.train_model:report:208 - Epoch 0 train 0.6706 test 0.6129 metric ['0.6725']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.73it/s]\n",
      "2023-05-31 14:07:58.697 | INFO     | src.models.train_model:report:208 - Epoch 1 train 0.5954 test 0.6166 metric ['0.6438']\n",
      "2023-05-31 14:07:58.699 | INFO     | src.models.train_model:__call__:245 - best loss: 0.6129396295547486, current loss 0.616637. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:28<00:00,  3.55it/s]\n",
      "2023-05-31 14:08:28.961 | INFO     | src.models.train_model:report:208 - Epoch 2 train 0.5268 test 0.4873 metric ['0.7638']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:33<00:00,  3.03it/s]\n",
      "2023-05-31 14:09:04.590 | INFO     | src.models.train_model:report:208 - Epoch 3 train 0.5055 test 0.4834 metric ['0.7788']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:40<00:00,  2.44it/s]\n",
      "2023-05-31 14:09:48.248 | INFO     | src.models.train_model:report:208 - Epoch 4 train 0.4892 test 0.4806 metric ['0.7700']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:31<00:00,  3.13it/s]\n",
      "2023-05-31 14:10:22.697 | INFO     | src.models.train_model:report:208 - Epoch 5 train 0.4467 test 0.4850 metric ['0.7588']\n",
      "2023-05-31 14:10:22.701 | INFO     | src.models.train_model:__call__:245 - best loss: 0.48055563688278197, current loss 0.485008. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.74it/s]\n",
      "2023-05-31 14:10:52.371 | INFO     | src.models.train_model:report:208 - Epoch 6 train 0.3990 test 0.4750 metric ['0.7600']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:24<00:00,  4.05it/s]\n",
      "2023-05-31 14:11:19.300 | INFO     | src.models.train_model:report:208 - Epoch 7 train 0.3984 test 0.4577 metric ['0.7788']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:27<00:00,  3.63it/s]\n",
      "2023-05-31 14:11:48.957 | INFO     | src.models.train_model:report:208 - Epoch 8 train 0.3837 test 0.5318 metric ['0.7937']\n",
      "2023-05-31 14:11:48.961 | INFO     | src.models.train_model:__call__:245 - best loss: 0.4577461540699005, current loss 0.531793. Counter 1.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:27<00:00,  3.70it/s]\n",
      "2023-05-31 14:12:18.077 | INFO     | src.models.train_model:report:208 - Epoch 9 train 0.3853 test 0.4849 metric ['0.7450']\n",
      "2023-05-31 14:12:18.079 | INFO     | src.models.train_model:__call__:245 - best loss: 0.4577461540699005, current loss 0.484868. Counter 2.000000/10.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [05:16<00:00, 31.69s/it]\n"
     ]
    }
   ],
   "source": [
    "attentionmodel = AttentionNLP()\n",
    "\n",
    "attentiontrainer = train_model.Trainer(\n",
    "    model=attentionmodel, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=trainstreamer, \n",
    "    validdataloader=teststreamer, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "\n",
    "attentiontrainer.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
