{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.data import data_tools, make_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import tokenizer, train_model\n",
    "import torch\n",
    "from src.models import metrics\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from src.models.rnn_models import NLPmodel, AttentionNLP\n",
    "import gin\n",
    "gin.parse_config_file(\"imdb.gin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:58:32.050 | INFO     | src.data.make_dataset:get_imdb_data:96 - ../../data/raw/aclImdb already exists, skipping download\n",
      "100%|██████████| 25000/25000 [00:46<00:00, 539.10it/s]\n",
      "100%|██████████| 25000/25000 [00:48<00:00, 512.69it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../data/raw\"\n",
    "trainpaths, testpaths = make_dataset.get_imdb_data(data_dir)\n",
    "traindataset = data_tools.TextDataset(paths=trainpaths)\n",
    "testdataset = data_tools.TextDataset(paths=testpaths)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:00:27.888 | INFO     | src.models.tokenizer:build_vocab:27 - Found 79808 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of vocab is 10002\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=10000)\n",
    "print(f\"Lenght of vocab is {len(v)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = tokenizer.Preprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "trainloader = DataLoader(\n",
    "    traindataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testdataset, collate_fn=preprocessor, batch_size=32, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset has 782 batches of 32 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup accuracy and loss_fn (this is a classification problem with two classes, 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "log_dir = Path(\"../../models/attention/\").resolve()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic config. We need to specify the vocabulary lenght for the embedding layer.\n",
    "Trainsteps are set to just 100 batches for speedup in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 10\n",
       "metrics: [Accuracy]\n",
       "logdir: /workspaces/ML22/models/attention\n",
       "train_steps: 100\n",
       "valid_steps: 25\n",
       "tunewriter: ['tensorboard', 'gin']\n",
       "optimizer_kwargs: {'lr': 0.001, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: {'save': False, 'verbose': True, 'patience': 10}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.settings import TrainerSettings\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=100,\n",
    "    valid_steps=25,\n",
    "    tunewriter=[\"tensorboard\", \"gin\"],\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No configurable matching 'rnn_models.AttentionNLP'.\n  In file \"imdb.gin\", line 9\n    rnn_models.AttentionNLP.config = {",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrnn_models\u001b[39;00m \u001b[39mimport\u001b[39;00m NLPmodel, AttentionNLP\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgin\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m gin\u001b[39m.\u001b[39;49mparse_config_file(\u001b[39m\"\u001b[39;49m\u001b[39mimdb.gin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39massert\u001b[39;00m gin\u001b[39m.\u001b[39mget_bindings(\u001b[39m\"\u001b[39m\u001b[39mNLPmodel\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(v)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:2450\u001b[0m, in \u001b[0;36mparse_config_file\u001b[0;34m(config_file, skip_unknown, print_includes_and_imports)\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[39mif\u001b[39;00m existence_check(config_file_with_prefix):\n\u001b[1;32m   2449\u001b[0m   \u001b[39mwith\u001b[39;00m reader(config_file_with_prefix) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m-> 2450\u001b[0m     includes, imports \u001b[39m=\u001b[39m parse_config(f, skip_unknown\u001b[39m=\u001b[39;49mskip_unknown)\n\u001b[1;32m   2451\u001b[0m     results \u001b[39m=\u001b[39m ParsedConfigFileIncludesAndImports(\n\u001b[1;32m   2452\u001b[0m         filename\u001b[39m=\u001b[39mconfig_file, imports\u001b[39m=\u001b[39mimports, includes\u001b[39m=\u001b[39mincludes)\n\u001b[1;32m   2453\u001b[0m     \u001b[39mif\u001b[39;00m print_includes_and_imports:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:2331\u001b[0m, in \u001b[0;36mparse_config\u001b[0;34m(bindings, skip_unknown)\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_skip(selector, skip_unknown):\n\u001b[1;32m   2330\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mtry_with_location(location):\n\u001b[0;32m-> 2331\u001b[0m       bind_parameter((scope, selector, arg_name), value)\n\u001b[1;32m   2332\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(statement, config_parser\u001b[39m.\u001b[39mBlockDeclaration):\n\u001b[1;32m   2333\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_skip(statement\u001b[39m.\u001b[39mselector, skip_unknown):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    135\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/utils.py:60\u001b[0m, in \u001b[0;36mtry_with_location\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exception, \u001b[39mSyntaxError\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m   \u001b[39mraise\u001b[39;00m  \u001b[39m# SyntaxErrors already include location information.\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m augment_exception_message_and_reraise(exception, _format_location(location))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/utils.py:56\u001b[0m, in \u001b[0;36mtry_with_location\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_with_location\u001b[39m(location):\n\u001b[1;32m     55\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     57\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exception, \u001b[39mSyntaxError\u001b[39;00m):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:2331\u001b[0m, in \u001b[0;36mparse_config\u001b[0;34m(bindings, skip_unknown)\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_skip(selector, skip_unknown):\n\u001b[1;32m   2330\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mtry_with_location(location):\n\u001b[0;32m-> 2331\u001b[0m       bind_parameter((scope, selector, arg_name), value)\n\u001b[1;32m   2332\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(statement, config_parser\u001b[39m.\u001b[39mBlockDeclaration):\n\u001b[1;32m   2333\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_skip(statement\u001b[39m.\u001b[39mselector, skip_unknown):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:1051\u001b[0m, in \u001b[0;36mbind_parameter\u001b[0;34m(binding_key, value)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m config_is_locked():\n\u001b[1;32m   1049\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempted to modify locked Gin config.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1051\u001b[0m pbk \u001b[39m=\u001b[39m ParsedBindingKey\u001b[39m.\u001b[39;49mparse(binding_key)\n\u001b[1;32m   1052\u001b[0m fn_dict \u001b[39m=\u001b[39m _CONFIG\u001b[39m.\u001b[39msetdefault(pbk\u001b[39m.\u001b[39mconfig_key, {})\n\u001b[1;32m   1053\u001b[0m fn_dict[pbk\u001b[39m.\u001b[39marg_name] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:910\u001b[0m, in \u001b[0;36mParsedBindingKey.parse\u001b[0;34m(cls, binding_key)\u001b[0m\n\u001b[1;32m    908\u001b[0m configurable_ \u001b[39m=\u001b[39m _parse_context()\u001b[39m.\u001b[39mget_configurable(selector)\n\u001b[1;32m    909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m configurable_:\n\u001b[0;32m--> 910\u001b[0m   _raise_unknown_configurable_error(selector)\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m configurable_\u001b[39m.\u001b[39mis_method \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m selector:\n\u001b[1;32m    913\u001b[0m   class_name \u001b[39m=\u001b[39m configurable_\u001b[39m.\u001b[39mselector\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/gin/config.py:648\u001b[0m, in \u001b[0;36m_raise_unknown_configurable_error\u001b[0;34m(selector)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_unknown_configurable_error\u001b[39m(selector):\n\u001b[0;32m--> 648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo configurable matching \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mselector\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No configurable matching 'rnn_models.AttentionNLP'.\n  In file \"imdb.gin\", line 9\n    rnn_models.AttentionNLP.config = {"
     ]
    }
   ],
   "source": [
    "\n",
    "assert gin.get_bindings(\"NLPmodel\")[\"config\"][\"vocab\"] == len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLPmodel(\n",
       "  (emb): Embedding(10002, 128)\n",
       "  (rnn): GRU(128, 128, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn_models.NLPmodel()\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base NLP model is just a GRU, with an embedding as a first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:53:52.365 | INFO     | src.data.data_tools:dir_add_timestamp:137 - Logging to /workspaces/ML22/models/attention/20230525-1653\n",
      "2023-05-25 16:53:52.401 | INFO     | src.models.train_model:__init__:109 - Found earlystop_kwargs in TrainerSettings. Set to None if you dont want earlystopping.\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:35<00:00,  2.78it/s]\n",
      "2023-05-25 16:54:31.162 | INFO     | src.models.train_model:report:207 - Epoch 0 train 0.6957 test 0.6962 metric ['0.5325']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:31<00:00,  3.18it/s]\n",
      "2023-05-25 16:55:04.937 | INFO     | src.models.train_model:report:207 - Epoch 1 train 0.6841 test 0.6861 metric ['0.5763']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:32<00:00,  3.08it/s]\n",
      "2023-05-25 16:55:41.187 | INFO     | src.models.train_model:report:207 - Epoch 2 train 0.6720 test 0.6653 metric ['0.6038']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:31<00:00,  3.22it/s]\n",
      "2023-05-25 16:56:14.549 | INFO     | src.models.train_model:report:207 - Epoch 3 train 0.6417 test 0.6269 metric ['0.6600']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:26<00:00,  3.72it/s]\n",
      "2023-05-25 16:56:43.226 | INFO     | src.models.train_model:report:207 - Epoch 4 train 0.5984 test 0.6126 metric ['0.6725']\n",
      " 41%|\u001b[38;2;30;71;6m████      \u001b[0m| 41/100 [00:10<00:15,  3.79it/s]\n",
      " 50%|\u001b[38;2;30;71;6m█████     \u001b[0m| 5/10 [03:01<03:01, 36.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m train_model\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m      3\u001b[0m     settings\u001b[39m=\u001b[39msettings, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[39m=\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[39m.\u001b[39;49mloop()\n",
      "File \u001b[0;32m/workspaces/ML22/notebooks/5_attention/../../src/models/train_model.py:126\u001b[0m, in \u001b[0;36mTrainer.loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    125\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mepochs), colour\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m#1e4706\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainbatches()\n\u001b[1;32m    127\u001b[0m         metric_dict, test_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevalbatches()\n\u001b[1;32m    128\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreport(epoch, train_loss, test_loss, metric_dict)\n",
      "File \u001b[0;32m/workspaces/ML22/notebooks/5_attention/../../src/models/train_model.py:152\u001b[0m, in \u001b[0;36mTrainer.trainbatches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraindataloader))\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 152\u001b[0m yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m    153\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(yhat, y)\n\u001b[1;32m    154\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspaces/ML22/notebooks/5_attention/../../src/models/rnn_models.py:116\u001b[0m, in \u001b[0;36mNLPmodel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    115\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x)\n\u001b[0;32m--> 116\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x)\n\u001b[1;32m    117\u001b[0m     last_step \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    118\u001b[0m     yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(last_step)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-jHmOY0S3-py3.9/lib/python3.9/site-packages/torch/nn/modules/rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    999\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m   1002\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = train_model.Trainer(\n",
    "    model=model, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=trainloader, \n",
    "    validdataloader=testloader, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "trainer.loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the impact of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"imdb.gin\")\n",
    "attentionmodel = rnn_models.AttentionNLP()\n",
    "attentiontrainer = train_model.Trainer(\n",
    "    model=attentionmodel, \n",
    "    settings=settings, \n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam, \n",
    "    traindataloader=trainloader, \n",
    "    validdataloader=testloader, \n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "attentiontrainer.loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9384df97cb25cd0ffeadd8ca5fc8c3b92d252d40e81804b4c63c6d046c91939e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
