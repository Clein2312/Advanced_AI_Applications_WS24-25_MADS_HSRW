{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.shapes import signature\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Input, output, Signatures\n",
    "Let's start with some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(5, 3), dtype:float64}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(loc=0, scale=0.1, size=(5, 3))\n",
    "signature(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trax has a `signature` function, which is almost similar to `.shape`, it's just more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(model, yhat):\n",
    "    print(f\"input: {model.n_in}\")\n",
    "    print(f\"output: {model.n_out}\")\n",
    "    print(f\"Signature: {signature(yhat)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a trax layer, for example a `Relu` layer, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0977031 ,  0.1033898 ,  0.00127924],\n",
       "        [-0.15047795, -0.041198  ,  0.02144017],\n",
       "        [-0.01992723, -0.06893442,  0.03013939],\n",
       "        [-0.08105862, -0.12708502,  0.04827876],\n",
       "        [-0.07423597, -0.054421  ,  0.19920343]]),\n",
       " DeviceArray([[0.0977031 , 0.10338981, 0.00127924],\n",
       "              [0.        , 0.        , 0.02144017],\n",
       "              [0.        , 0.        , 0.03013939],\n",
       "              [0.        , 0.        , 0.04827876],\n",
       "              [0.        , 0.        , 0.19920343]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = tl.Relu()\n",
    "yhat = relu(X)\n",
    "X, yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 3), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "print_info(relu, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has one input, one output, and the shape is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 6), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "concat = tl.Concatenate()\n",
    "yhat = concat([X, X])\n",
    "print_info(concat, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate will take two inputs, and will merge them into one.\n",
    "We can tell concatenate to take three inputs as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 9), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "concat3 = tl.Concatenate(n_items=3)\n",
    "yhat = concat3([X, X, X])\n",
    "print_info(concat3, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to use another axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(10, 3), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "concat = tl.Concatenate(axis=0)\n",
    "yhat = concat([X, X])\n",
    "print_info(concat, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Combinators\n",
    "The most interesting part of trax are the combinators. \n",
    "## 2.1 Sequential\n",
    "The serial layer is similar to the `sequential` from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.layers import combinators as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 128), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model1 = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    ")\n",
    "model1.init_weights_and_state(signature(X))\n",
    "yhat = model1(X)\n",
    "print_info(model1, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we dont have to specify the size of the input. \n",
    "Calling `.init_weights_and_state` on a specific signature will infer the inputs sizes needed to make things work.\n",
    "\n",
    "Expanding the model is as simple as adding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 32), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model2 = cb.Serial(\n",
    "    tl.Dense(64),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(32),\n",
    "    tl.Relu(),\n",
    ")\n",
    "\n",
    "model2.init_weights_and_state(signature(X))\n",
    "yhat = model2(X)\n",
    "print_info(model2, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Branch combinator\n",
    "With `torch` we have seen skip layers. With vanilla Python, could do that like this:\n",
    "\n",
    "```python\n",
    "...\n",
    "def forward(self, x):\n",
    "    # torch implementation\n",
    "    skip = x\n",
    "    x = self.neuralnetwork(x)\n",
    "    out = skip + x\n",
    "    return out\n",
    "```\n",
    "\n",
    "We have also seen parallel processing, e.g. with the GoogleNet architecture, sort of like this:\n",
    "\n",
    "```python\n",
    "...\n",
    "def forward(self, x):\n",
    "    # torch implementation\n",
    "    x1 = self.conv1(x)\n",
    "    x2 = self.conv2(x)\n",
    "    out = self.concat(x1, x2)\n",
    "    return out\n",
    "```\n",
    "\n",
    "However, with `trax`, we can use `Branch` to make parallel branches.\n",
    "Trax uses a stack of inputs. With `Branch`, each layer consumes as much inputs from the stack as needed.\n",
    "\n",
    "For example, suppose one has three layers:\n",
    "\n",
    "    - F: 1 input, 1 output\n",
    "    - G: 3 inputs, 1 output\n",
    "    - H: 2 inputs, 2 outputs (h1, h2)\n",
    "\n",
    "Branch(F, G) will take three inputs in parallel, and give 2 outputs.\n",
    "Every function just takes from the stack what it needs, with a maximum of three inputs.\n",
    "So with input (a, b, c), we will have F(a) and G(a, b, c).\n",
    "\n",
    "Branch(F, G, H) will take 3 inputs and give 4 outputs:\n",
    "\n",
    "    - inputs: a, b, c\n",
    "    - outputs: F(a), G(a, b, c), H(a, b) -> f1, g1, h1, h2 \n",
    "\n",
    "### Example of Branch\n",
    "Above, we created two Neural Networks, model1 and model2.\n",
    "Let's say we want to take one input, and branch it through the two models in parallel.\n",
    "\n",
    "<img src=\"../../reports/figures/parallel.png\"/>\n",
    "\n",
    "We need a model that:\n",
    "- takes 1 input x.\n",
    "- This input is processed as model1(x), model2(x)\n",
    "- model1 outputs (batch, 128) while model2 outputs (batch, 32)\n",
    "- model1 and model2 both have one output, so the output is m1, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 2\n",
      "Signature: (ShapeDtype{shape:(5, 128), dtype:float32}, ShapeDtype{shape:(5, 32), dtype:float32})\n"
     ]
    }
   ],
   "source": [
    "model = cb.Serial(cb.Branch(model1, model2))\n",
    "model.init_weights_and_state(signature(X))\n",
    "yhat = model(X)\n",
    "print_info(model, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we merge those two outputs by using a function like concatenate, that takes two inputs and outputs a single matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 160), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model = cb.Serial(cb.Branch(model1, model2), cb.Concatenate())\n",
    "model.init_weights_and_state(signature(X))\n",
    "yhat = model(X)\n",
    "print_info(model, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we leave one item empty, like `cb.Branch([], model)`, one copy of `x` is simply passed through without being processed.   \n",
    "This is usefull for skiplayers, like the residual architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(32, 10, 128), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(32, 10, 128)\n",
    "\n",
    "dnn = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    ")\n",
    "\n",
    "\n",
    "residual = cb.Serial(cb.Branch([], dnn), cb.Add())\n",
    "\n",
    "residual.init_weights_and_state(signature(X))\n",
    "yhat = residual(X)\n",
    "print_info(residual, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Parallel combinator\n",
    "First, have a look at the embedding layer we already have seen in `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(32, 10, 16), dtype:float32}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = tl.Embedding(vocab_size=1000, d_feature=16)\n",
    "X = np.random.randint(0, 1000, size=(32, 10))\n",
    "emb.init_weights_and_state(signature(X))\n",
    "X_ = emb(X)\n",
    "signature(X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really similar to `torch`. From the torch documentation: \n",
    "\n",
    "> torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
    "\n",
    "In Trax, `num_embeddings` and `embedding_dim` just have different names: `vocab_size` and `d_feature`.\n",
    "Not much news here.\n",
    "\n",
    "### parallel embeddings usecase\n",
    "But let us have a medical example where we would need three embeddings.\n",
    "Let there be three categorical inputs:\n",
    "1. 20 different types of medication\n",
    "2. 1000 different medical diagnoses\n",
    "3. 128 different locations where patients are treated.\n",
    "\n",
    "We would need three embedding layers, each with a different `vocab_size`. With `torch`, implementing this would be a bit more complex, especially if you want your model to be flexible enough to be able to use *any* number of embedding layers that you specify at the start of the model with a parameter.\n",
    "\n",
    "One approach in `torch` would be to create a `ModuleDict`, where you can collect multiple layers with a name, keep track of every name-layer pair, and call the right layer when needed.\n",
    "\n",
    "Just to let you appreciate the simplicity of `trax`, here is just a part of an implementation of a multiembedding in pytorch-forecasting. I removed parts of the code at the place of the dots `...`, for simplicity. The only thing I want you to take away from this example is that it is fairly complex and takes a lot of code.\n",
    "\n",
    "```python\n",
    "class MultiEmbedding(nn.Module):\n",
    "    ...\n",
    "    def init_embeddings(self):\n",
    "            self.embeddings = nn.ModuleDict()\n",
    "            for name in self.embedding_sizes.keys():\n",
    "                embedding_size = self.embedding_sizes[name][1]\n",
    "                ...\n",
    "                # convert to list to become mutable\n",
    "                self.embedding_sizes[name] = list(self.embedding_sizes[name])\n",
    "                self.embedding_sizes[name][1] = embedding_size\n",
    "                ...\n",
    "                    self.embeddings[name] = nn.Embedding(\n",
    "                        self.embedding_sizes[name][0],\n",
    "                        embedding_size,\n",
    "                        padding_idx=padding_idx,\n",
    "                    )\n",
    "    ...\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "    input_vectors = {}\n",
    "            for name, emb in self.embeddings.items():\n",
    "                if name in self.categorical_groups:\n",
    "                    input_vectors[name] = emb(\n",
    "                        x[\n",
    "                            ...,\n",
    "                            [self.x_categoricals.index(cat_name) for cat_name in self.categorical_groups[name]],\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    input_vectors[name] = emb(x[..., self.x_categoricals.index(name)])\n",
    "```\n",
    "\n",
    "You can look up the full implementation at [github](https://github.com/jdb78/pytorch-forecasting/blob/master/pytorch_forecasting/models/nn/embeddings.py#L32), which is 163 lines long!\n",
    "\n",
    "\n",
    "To do this in `trax`, we will use the `Parallel` layer. From the [source code](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L138) of `trax`:\n",
    "> For example, suppose one has three layers:\n",
    ">    - F: 1 input, 1 output\n",
    ">    - G: 3 inputs, 1 output\n",
    ">    - H: 2 inputs, 2 outputs (h1, h2)\n",
    ">\n",
    ">  Then Parallel(F, G, H) will take 6 inputs and give 4 outputs:\n",
    ">\n",
    ">    - inputs: a, b, c, d, e, f\n",
    ">    - outputs: F(a), G(b, c, d), h1, h2     where h1, h2 = H(e, f)\n",
    "\n",
    "This is almost similar to `Branch`, but it does not duplicate inputs but will just consume the stack.\n",
    "\n",
    "Now, have a look at the `trax` implementation of the multiembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we set up vocab sizes and some random input\n",
    "vocab_sizes = [20, 1000, 128]\n",
    "input = [np.random.randint(0, v, size=(32, 10)) for v in vocab_sizes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiembedding(vocab_sizes):\n",
    "    embeddings = [tl.Embedding(vocab_size=vocab, d_feature=16) for vocab in vocab_sizes]\n",
    "\n",
    "    model = cb.Serial(cb.Parallel(*embeddings))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3\n",
      "output: 3\n",
      "Signature: (ShapeDtype{shape:(32, 10, 16), dtype:float32}, ShapeDtype{shape:(32, 10, 16), dtype:float32}, ShapeDtype{shape:(32, 10, 16), dtype:float32})\n"
     ]
    }
   ],
   "source": [
    "model = multiembedding(vocab_sizes)\n",
    "model.init_weights_and_state(signature(input))\n",
    "yhat = model(input)\n",
    "print_info(model, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope to have convinced you that `trax` makes writing models simpler, more elegant, better to read and faster."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
