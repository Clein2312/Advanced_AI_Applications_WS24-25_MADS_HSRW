{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.shapes import signature\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inupt, output, Signatures\n",
    "Let's start with some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(5, 3), dtype:float64}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(loc=0, scale=0.1, size=(5,3))\n",
    "signature(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trax has a `signature` function, which is almost similar to `.shape`, it's just more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(model, yhat):\n",
    "    print(f\"input: {model.n_in}\")\n",
    "    print(f\"output: {model.n_out}\")\n",
    "    print(f\"Signature: {signature(yhat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a trax layer, for example a `Relu` layer, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.08077762, -0.07764351,  0.11378873],\n",
       "        [ 0.20770353, -0.07594232, -0.04436042],\n",
       "        [ 0.03507322, -0.10389911, -0.05197999],\n",
       "        [-0.12117449,  0.03699227,  0.12006499],\n",
       "        [-0.02152088,  0.05857737, -0.00227104]]),\n",
       " DeviceArray([[0.        , 0.        , 0.11378873],\n",
       "              [0.20770353, 0.        , 0.        ],\n",
       "              [0.03507322, 0.        , 0.        ],\n",
       "              [0.        , 0.03699227, 0.120065  ],\n",
       "              [0.        , 0.05857737, 0.        ]], dtype=float32))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = tl.Relu()\n",
    "yhat = relu(X)\n",
    "X, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 3), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "print_info(relu, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has one input, one output, and the shape is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 6), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "concat = tl.Concatenate()\n",
    "yhat = concat([X, X])\n",
    "print_info(concat, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate will take two inputs, and will merge them into one.\n",
    "We can tell concatenate to take three inputs as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 9), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "concat3 = tl.Concatenate(n_items=3)\n",
    "yhat = concat3([X, X, X])\n",
    "print_info(concat3, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinators\n",
    "The most interesting part of trax are the combinators. The serial layer is similar to the `sequential` from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.layers import combinators as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 128), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model1 = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    ")\n",
    "model1.init_weights_and_state(signature(X))\n",
    "yhat = model1(X)\n",
    "print_info(model1, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we dont have to specify the size of the input. \n",
    "Calling `.init_weights_and_state` on a specific signature will infer the inputs sizes needed to make things work.\n",
    "\n",
    "Expanding the model is as simple as adding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 32), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model2 = cb.Serial(\n",
    "    tl.Dense(64),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(32),\n",
    "    tl.Relu(),\n",
    ") \n",
    "model2.init_weights_and_state(signature(X))\n",
    "yhat = model2(X)\n",
    "print_info(model2, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branch combinator\n",
    "With `torch` we have seen skip layers. We would do that like this:\n",
    "\n",
    "```python\n",
    "...\n",
    "def forward(self, x):\n",
    "    # torch implementation\n",
    "    skip = x\n",
    "    x = self.neuralnetwork(x)\n",
    "    out = skip + x\n",
    "    return out\n",
    "```\n",
    "\n",
    "We have also seen parallel processing, sort of like this:\n",
    "\n",
    "```python\n",
    "...\n",
    "def forward(self, x):\n",
    "    # torch implementation\n",
    "    x1 = self.conv1(x)\n",
    "    x2 = self.conv2(x)\n",
    "    out = self.concat(x1, x2)\n",
    "    return out\n",
    "```\n",
    "\n",
    "However, with `trax`, we can use `Branch` to make parallel branches.\n",
    "Trax uses a stack of inputs. With `Branch`, each layer consumes as much inputs from the stack as needed.\n",
    "\n",
    "For example, suppose one has three layers:\n",
    "\n",
    "    - F: 1 input, 1 output\n",
    "    - G: 3 inputs, 1 output\n",
    "    - H: 2 inputs, 2 outputs (h1, h2)\n",
    "\n",
    "Then Branch(F, G, H) will take 3 inputs and give 4 outputs:\n",
    "\n",
    "    - inputs: a, b, c\n",
    "    - outputs: F(a), G(a, b, c), H(a, b) -> f1, g1, h1, h2 \n",
    "\n",
    "The next model:\n",
    "- takes 1 input x.\n",
    "- This input is processed as model1(x), model2(x)\n",
    "- model1 and model2 both have one output, so the output is m1, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 2\n",
      "Signature: (ShapeDtype{shape:(5, 128), dtype:float32}, ShapeDtype{shape:(5, 32), dtype:float32})\n"
     ]
    }
   ],
   "source": [
    "model = cb.Serial(\n",
    "    cb.Branch(model1, model2)\n",
    ")\n",
    "model.init_weights_and_state(signature(X))\n",
    "yhat = model(X)\n",
    "print_info(model, yhat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we could merge those two outputs by using a function like concatenate, that takes two inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(5, 160), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "model = cb.Serial(\n",
    "    cb.Branch(model1, model2),\n",
    "    cb.Concatenate()\n",
    ")\n",
    "model.init_weights_and_state(signature(X))\n",
    "yhat = model(X)\n",
    "print_info(model, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we leave one item empty, like `cb.Branch([], model)`, one copy of `x` is simply passed through without being processed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1\n",
      "output: 1\n",
      "Signature: ShapeDtype{shape:(32, 10, 128), dtype:float32}\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(32, 10, 128)\n",
    "\n",
    "dnn = cb.Serial(\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    "    tl.Dense(128),\n",
    "    tl.Relu(),\n",
    ") \n",
    "\n",
    "\n",
    "residual = cb.Serial(\n",
    "    cb.Branch([], dnn),\n",
    "    cb.Add()\n",
    ")\n",
    "\n",
    "residual.init_weights_and_state(signature(X))\n",
    "yhat = residual(X)\n",
    "print_info(residual, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel combinator\n",
    "First, have a look at the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(32, 10, 16), dtype:float32}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = tl.Embedding(vocab_size=1000, d_feature=16)\n",
    "X = np.random.randint(0, 1000, size=(32, 10))\n",
    "emb.init_weights_and_state(signature(X))\n",
    "X_ = emb(X)\n",
    "signature(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really similar to `torch`. We need to define a vocab_size, and a dimensionality of the output. Nothing new here.\n",
    "\n",
    "But let us have a medical example where we would need three embeddings.\n",
    "Let there be three categorical inputs:\n",
    "1. 20 different types of medication\n",
    "2. 1000 different medical diagnoses\n",
    "3. 128 different locations where patients are treated.\n",
    "\n",
    "We would need three embedding layers, each with a different vocab_size. With `torch`, implementing this would be complex, especially if you want your model to be flexible enough to use any number of embedding layers that you specify at the start of the model with a config.\n",
    "\n",
    "One approach in `torch` would be to create a `ModuleDict`, where you can collect multiple layers with a name, keep track of every name-layer pair, and call the right layer when needed.\n",
    "\n",
    "Just to let you appreciate the simplicity of `trax`, here is just a part of an implementation of a multiembedding in pytorch-forecasting. I removed parts of the code at the place of the dots, for simplicity. The only thing I want you to take away from this example is that it is fairly complex and takes a lot of code.\n",
    "\n",
    "```python\n",
    "class MultiEmbedding(nn.Module):\n",
    "    ...\n",
    "    def init_embeddings(self):\n",
    "            self.embeddings = nn.ModuleDict()\n",
    "            for name in self.embedding_sizes.keys():\n",
    "                embedding_size = self.embedding_sizes[name][1]\n",
    "                ...\n",
    "                # convert to list to become mutable\n",
    "                self.embedding_sizes[name] = list(self.embedding_sizes[name])\n",
    "                self.embedding_sizes[name][1] = embedding_size\n",
    "                ...\n",
    "                    self.embeddings[name] = nn.Embedding(\n",
    "                        self.embedding_sizes[name][0],\n",
    "                        embedding_size,\n",
    "                        padding_idx=padding_idx,\n",
    "                    )\n",
    "    ...\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "    input_vectors = {}\n",
    "            for name, emb in self.embeddings.items():\n",
    "                if name in self.categorical_groups:\n",
    "                    input_vectors[name] = emb(\n",
    "                        x[\n",
    "                            ...,\n",
    "                            [self.x_categoricals.index(cat_name) for cat_name in self.categorical_groups[name]],\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    input_vectors[name] = emb(x[..., self.x_categoricals.index(name)])\n",
    "```\n",
    "\n",
    "You can look up the full implementation at [github](https://github.com/jdb78/pytorch-forecasting/blob/master/pytorch_forecasting/models/nn/embeddings.py#L32), which is 163 lines long!\n",
    "\n",
    "\n",
    "To do this in `trax`, we will use the `Parallel` layer. From the [source code](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L138) of `trax`:\n",
    "> For example, suppose one has three layers:\n",
    ">    - F: 1 input, 1 output\n",
    ">    - G: 3 inputs, 1 output\n",
    ">    - H: 2 inputs, 2 outputs (h1, h2)\n",
    ">\n",
    ">  Then Parallel(F, G, H) will take 6 inputs and give 4 outputs:\n",
    ">\n",
    ">    - inputs: a, b, c, d, e, f\n",
    ">    - outputs: F(a), G(b, c, d), h1, h2     where h1, h2 = H(e, f)\n",
    "\n",
    "This is almost similar to `Branch`, but it does not duplicate inputs but will just consume the stack.\n",
    "\n",
    "Now, have a look at the `trax` implementation of the multiembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we set up vocab sizes and some random input\n",
    "vocab_sizes = [20, 1000, 128]\n",
    "input = [np.random.randint(0, v, size=(32, 10)) for v in vocab_sizes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiembedding(vocab_sizes):\n",
    "    embeddings = [tl.Embedding(vocab_size=vocab, d_feature=16) for vocab in vocab_sizes]\n",
    "\n",
    "    model = cb.Serial(\n",
    "        cb.Parallel(*embeddings)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 3\n",
      "output: 3\n",
      "Signature: (ShapeDtype{shape:(32, 10, 16), dtype:float32}, ShapeDtype{shape:(32, 10, 16), dtype:float32}, ShapeDtype{shape:(32, 10, 16), dtype:float32})\n"
     ]
    }
   ],
   "source": [
    "model = multiembedding(vocab_sizes)\n",
    "model.init_weights_and_state(signature(input))\n",
    "yhat = model(input)\n",
    "print_info(model, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope to have convinced you that `trax` makes writing models simpler, more elegant, better to read and faster."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
