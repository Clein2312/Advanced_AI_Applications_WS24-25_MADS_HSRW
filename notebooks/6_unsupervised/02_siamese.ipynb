{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from trax import layers as tl\n",
    "from trax.shapes import signature\n",
    "from trax.layers import combinators as cb\n",
    "from trax.layers.assert_shape import assert_shape\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src.models.build import summary\n",
    "from pathlib import Path\n",
    "from src.data import data_tools\n",
    "\n",
    "from src.settings import SiameseSettings\n",
    "\n",
    "settings = SiameseSettings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.80M/3.80M [00:01<00:00, 3.46MiB/s]\n"
     ]
    }
   ],
   "source": [
    "url = settings.url\n",
    "data_dir = settings.data_dir\n",
    "data_tools.get_file(data_dir, settings.file, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "paths, _ = data_tools.iter_valid_paths(data_dir / settings.training, formats=[\".pgm\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at a file.\n",
    "This dataset consists of faces of 40 subjects, and every subject has 10 photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (112, 92)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PpmImagePlugin.PpmImageFile image mode=L size=92x112>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = next(paths)\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(file)\n",
    "print(f\"Shape: {np.array(img).shape}\")\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are shaped (112, 92, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 112, 92, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "X = rng.random((32,) + np.array(img).shape + (1,))\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we will do is build a siamese network.\n",
    "\n",
    "Why would you want to do that? \n",
    "## Case 1\n",
    "Well, let's say you want to train an image model that is used for a safety camera. \n",
    "You would want to know: is this person entering the building the same person as is shown on the passport?\n",
    "\n",
    "A human can compare the two images, but how about a model?\n",
    "We can't train a model to classify all human faces into one of 7 billion classes, so classification does not seem to be the way to go.\n",
    "\n",
    "So, how to solve this?\n",
    "## Case 2\n",
    "You want a model that checks if a signature of a person matches the signature you have in your database. Again, you could do classification but you will quickly run into problems; what if you have 10.000 signatures?\n",
    "\n",
    "For cases like this, we can train a siamese model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/11/keras_siamese_networks_header.png?lossy=1&strip=1&webp=1\" />\n",
    "\n",
    "The idea of the siamese model is to have an encoder that transforms images into vectors.\n",
    "\n",
    "We then compare the two vectors. If the two faces, signatures, soundbytes, birdsongs, etc. etc. are the same, the embedding be close to each other.\n",
    "If they are not the same, the should be far away.\n",
    "\n",
    "This is a slight variation on the autoencoder that also has to figure out clusters, but now we don't need to reconstruct an image. We just want the embedding to discern unique characteristics of a face, signature, etc.\n",
    "\n",
    "Our basemodel is a basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer                   input                dtype     output               dtype \n",
      "(0) Conv                (32, 112, 92, 1)   (float64) | (32, 37, 30, 96)   (float32)\n",
      "(1) Relu                (32, 37, 30, 96)   (float32) | (32, 37, 30, 96)   (float32)\n",
      "(2) Conv                (32, 37, 30, 96)   (float32) | (32, 18, 14, 256)  (float32)\n",
      "(3) Relu                (32, 18, 14, 256)  (float32) | (32, 18, 14, 256)  (float32)\n",
      "(4) Conv                (32, 18, 14, 256)  (float32) | (32, 8, 6, 384)    (float32)\n",
      "(5) Relu                (32, 8, 6, 384)    (float32) | (32, 8, 6, 384)    (float32)\n",
      "(6) Avg2D               (32, 8, 6, 384)    (float32) | (32, 384)          (float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(32, 384), dtype:float32}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Avg2D():\n",
    "    return tl.Fn(\"Avg2D\", lambda x: x.mean(axis=(1, 2)), n_out=1)\n",
    "\n",
    "\n",
    "@assert_shape(\"bwhc->bd\")\n",
    "def Encoder(config) -> cb.Serial:\n",
    "    model = cb.Serial(\n",
    "        tl.Conv(config[\"filters1\"], kernel_size=(3, 3), strides=(3, 3)),\n",
    "        tl.Relu(),\n",
    "        tl.Conv(config[\"filters2\"], kernel_size=(3, 3), strides=(2, 2)),\n",
    "        tl.Relu(),\n",
    "        tl.Conv(config[\"filters3\"], kernel_size=(3, 3), strides=(2, 2)),\n",
    "        tl.Relu(),\n",
    "        Avg2D(),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"filters1\": 96,\n",
    "    \"filters2\": 256,\n",
    "    \"filters3\": 384,\n",
    "}\n",
    "encoder = Encoder(config)\n",
    "encoder.init_weights_and_state(signature(X))\n",
    "summary(encoder, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can check, we start out with a batch of 32 images, each (112,92) big, with 1 channel.\n",
    "Our model has three layers, and transforms the images into vectors of lenght 384.\n",
    "At the final step, we have feature maps of (8,6) pixels and we just take the average activation of those maps.\n",
    "\n",
    "The idea of the siamese model is to calculate the distance between the two.\n",
    "We will subtract and calculate the L2 norm, $$\\sqrt{||x_1 - x_2||^2}$$\n",
    "\n",
    "This is a generalized form of the Pythagoras formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99638569, 1.36652624, 1.48942232, 0.94027443, 1.21886967,\n",
       "       1.26840181, 1.15881783, 1.13957163, 0.86302176, 0.91448523,\n",
       "       1.22596086, 1.09914518, 0.87619859, 1.21851909, 1.10739297,\n",
       "       1.41997306, 1.32995764, 1.30953545, 0.93793056, 0.85151571,\n",
       "       1.22637958, 0.96671051, 1.05905192, 1.66361459, 1.12266768,\n",
       "       1.35447261, 1.47397101, 1.68142885, 1.23346268, 0.95225442,\n",
       "       0.80216615, 1.10539865])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = rng.random((32, 8))\n",
    "x2 = rng.random((32, 8))\n",
    "yhat = rng.integers(0, 2, 32)\n",
    "distance = np.linalg.norm(x1 - x2, axis=-1)\n",
    "distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is:\n",
    "we want the images from the same class (the same person) to be close to each other, and images from different persons to be far away.\n",
    "\n",
    "We do this with contrastive loss:\n",
    "$$\\hat{y} * d + (1-\\hat{y}) * max(0, m - d)$$\n",
    "\n",
    "where $m$ is the margin we choose as \"close\", so let's take a distance of 1.0.\n",
    "Let's implement this as a proper loss class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.6975856, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ContrastiveLoss:\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        self.margin = margin\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"ContrastiveLoss\"\n",
    "\n",
    "    def __call__(self, y1, y2, yhat):\n",
    "        distance = jnp.linalg.norm(y1 - y2, axis=-1)\n",
    "        # this prevents to distance to become too close to zero, or too far away\n",
    "        distance_ = jnp.clip(distance, 1e-14, 10)\n",
    "        return jnp.mean(\n",
    "            yhat * distance + (1 - yhat) * jnp.maximum(0, self.margin - distance)\n",
    "        )\n",
    "\n",
    "\n",
    "loss = ContrastiveLoss()\n",
    "loss(x1, x2, yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With trax, creating a Siamese network is a piece of cake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(config):\n",
    "    encoder = Encoder(config)\n",
    "    model = cb.Parallel(encoder, encoder)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShapeDtype{shape:(32, 384), dtype:float32},\n",
       " ShapeDtype{shape:(32, 384), dtype:float32})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = rng.random((32,) + np.array(img).shape + (1,))\n",
    "img2 = rng.random((32,) + np.array(img).shape + (1,))\n",
    "y = rng.random(32)\n",
    "siamese = Siamese(config)\n",
    "siamese.init_weights_and_state(signature((img1, img2)))\n",
    "x1, x2 = siamese((img1, img2))\n",
    "signature((x1, x2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.4690957, dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x1, x2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do is to create a proper dataloader, and to wrap everything in a trainloop.\n",
    "\n",
    "Another strategy would be to use transfer learing;\n",
    "e.g. if we load a pretrained resnet from torchvision, and slap on a final Dense layer, \n",
    "our training is probably faster and more accurate."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15911aab0965639e9482f052beb89e7ca291bb3f153727c5758e3fe9ad1321e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deep-learning-xB8KIJr7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
