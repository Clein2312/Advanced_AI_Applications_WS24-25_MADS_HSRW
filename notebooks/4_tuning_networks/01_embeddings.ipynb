{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start with a simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'the cat sat on the mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How to encode this? Lets start with OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "oh = OneHotEncoder()\n",
    "X = np.array(text.split(\" \")).reshape(-1, 1)\n",
    "X = oh.fit_transform(X)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use one hot encoding, like this\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/guide/images/one-hot.png width=400/>\n",
    "\n",
    "But this becomes really inefficient if we have a 10.000 word vocabulary....\n",
    "\n",
    "So, maybe let's give each word a unique number. First, we create a tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 16:51:47.269466: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X = text.split(\" \")\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to set some hyperparameters for the creation of our embedding. \n",
    "\n",
    "- What is the **maximum vocabulary size**? This depenends on the problem at hand and the vocabulary available.\n",
    "- What is a **max sequence length** at which we want to cut off the sequences? We wont be using RNNs, so we have to set a fixed length. If a sequence is too short, it will be padded with some token, eg zeros.\n",
    "- What is the **dimensionality of the embedding**? This depends on the problem at hand. If we create an embedding with 50 dimensions, it can contain much more finegrained information as opposed to a 10D embedding, or 2D. But embeddings of 50 (or 300, or 1000) might also become too complex, and thus suffer from overfitting / long time to train etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5 + 2  # Maximum vocab size . 5 for the words, 2 for a padding token and an unknown token.\n",
    "max_len = 6  # max sequence length to pad the outputs to.\n",
    "embedding_dims = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these settings, we can initialize the vectorization of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = TextVectorization(\n",
    " max_tokens=vocab_size,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the vocab layer has been created, call `adapt` on the text-only dataset to create the vocabulary. You don't have to batch with this small example, but for large datasets this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_dataset.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, those were all the steps we had to take. We now have a functioning vectorize layer. Let's have a look at the resulting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'sat', 'on', 'mat', 'cat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognize the words of the text, but also additional tokens for an empty token (used for padding) and an unknown token.\n",
    "\n",
    "Let's look at a minimal example of the vectorization in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([2, 6, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the layer works as a stand alone layer to vectorize strings\n",
    "vectorize_layer(\"the cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Embedding\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "model = Sequential([\n",
    "    # Start by creating an explicit input layer. It needs to have a shape of\n",
    "    # (1,) (because we need to tell the model that there is exactly one string\n",
    "    # input per batch), and the dtype needs to be 'string'.\n",
    "    InputLayer(input_shape=[1], dtype=tf.string),\n",
    "    # The first layer in our model is the vectorization layer. After this\n",
    "    # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "    # indices.\n",
    "    vectorize_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 2 6 0 0]\n",
      " [2 6 3 4 2 5]]\n"
     ]
    }
   ],
   "source": [
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"where sat the cat\"], [\"the cat sat on the mat\"]]\n",
    "output = model.predict(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is what we wanted! Instead of using a one-hot-encoding that takes a lot of space, we have a sparse encoding that encodes every word as a arbitrary integer.\n",
    "\n",
    "Can you trace back the sentence encoding from the vocabulary?\n",
    "Try to decode for yourself. Look at the sentence 'where sat the cat', look at the vocabulary we printed three cells back, and look at the result `[1,3,2,6,0,0]`.\n",
    "\n",
    "- what happened with the word 'where'? Why is that?\n",
    "- what happened with the length of the inputsentence?\n",
    "\n",
    "\n",
    "We can feed this vectorized form to an embedding layer in the shape of `(batches x sequencelength)`. An Embedding layer will try to learn an encoding of the input. The learning will take place with regards to the output we feed the model, and the loss function.\n",
    "\n",
    "The output of the Embedding layer will be `(batches x sequencelength x embedding_dimensionality)`. So, 32 sentences, each with a length of 10 words, will have a shape `(32,10)`. The ouput of the embedding will be `(32, 10, 4)` if you use a 4-dimensional embedding for every word.\n",
    "\n",
    "<img src=https://www.tensorflow.org/text/guide/images/embedding2.png width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 5\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=[1], dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    Embedding(input_dim = vocab_size, output_dim = embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(input_data)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see:\n",
    "\n",
    "- we have two observations (batch size)\n",
    "- every observation has a sequence length of 6 words\n",
    "- every word is encoded with 5 numbers\n",
    "\n",
    "We have effectively embedded every word in a 5 dimensional vectorspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00285207,  0.01483097,  0.00756167,  0.00207195,  0.02628548],\n",
       "       [ 0.03857994, -0.0447681 ,  0.01171069, -0.03995188, -0.00779179],\n",
       "       [ 0.00806916, -0.02925973,  0.04288353,  0.02198106,  0.01243716],\n",
       "       [ 0.00224929, -0.016835  ,  0.01042606, -0.02297521,  0.04011783],\n",
       "       [-0.03304671,  0.04753392, -0.02972338,  0.014785  ,  0.00142245],\n",
       "       [-0.03304671,  0.04753392, -0.02972338,  0.014785  ,  0.00142245]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0] # result for the first sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We fed the model a sentence of 6 words, which were 6 numbers. The output is a vector of 5 numbers for every word, so (6 x 5) numbers. You can look at this output as a way of generating features. For that part, it is similar to what we have been doing with `Conv1D` and `Conv2D` layers.\n",
    "\n",
    "Another way to think about these embeddings, is as en encoding of the \"meaning\" of every word."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd838b4e8e084a6a759aff6a24fadbf2445d28874d00da48c8398869c390159d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
